[{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['283','0','467','0','Untitled-1',"scenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:01.489Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['284','0','467','0','Untitled-1',"    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:01.789Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['285','0','467','0','Untitled-1',"    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:01.990Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['286','0','467','0','Untitled-1',"    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:02.124Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['287','0','467','0','Untitled-1',"    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:02.174Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['288','0','467','0','Untitled-1',"}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:02.223Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['289','0','467','0','Untitled-1',"\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:02.323Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['288','0','467','0','Untitled-1',"}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:02.540Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['287','0','467','0','Untitled-1',"    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:02.557Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['285','0','467','0','Untitled-1',"    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:02.573Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['284','0','467','0','Untitled-1',"    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:02.590Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['283','0','467','0','Untitled-1',"scenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:02.623Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['282','0','467','0','Untitled-1',"# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:02.773Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['281','0','467','0','Untitled-1',"\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:02.823Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['279','0','467','0','Untitled-1',"\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:03.141Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['280','0','306','0','Untitled-1'],time:'2024-09-05T08:20:03.156Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['279','0','306','0','Untitled-1'],time:'2024-09-05T08:20:03.173Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['278','0','467','0','Untitled-1',"    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:03.177Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['279','0','305','91','Untitled-1'],time:'2024-09-05T08:20:03.189Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['278','0','305','91','Untitled-1'],time:'2024-09-05T08:20:03.206Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['277','0','467','0','Untitled-1',"    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:03.210Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['277','0','304','45','Untitled-1'],time:'2024-09-05T08:20:03.222Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['276','0','467','0','Untitled-1',"\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:03.244Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['276','0','303','53','Untitled-1'],time:'2024-09-05T08:20:03.255Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['275','0','467','0','Untitled-1',"    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:03.258Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['276','0','302','0','Untitled-1'],time:'2024-09-05T08:20:03.272Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['275','0','302','0','Untitled-1'],time:'2024-09-05T08:20:03.289Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['274','0','467','0','Untitled-1',"\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:03.292Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['275','0','301','40','Untitled-1'],time:'2024-09-05T08:20:03.306Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['274','0','301','40','Untitled-1'],time:'2024-09-05T08:20:03.322Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['273','0','467','0','Untitled-1',"    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:03.325Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['273','0','300','64','Untitled-1'],time:'2024-09-05T08:20:03.339Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['272','0','467','0','Untitled-1',"\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:03.342Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['273','0','299','0','Untitled-1'],time:'2024-09-05T08:20:03.356Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['272','0','299','0','Untitled-1'],time:'2024-09-05T08:20:03.372Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['271','0','467','0','Untitled-1',"    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:03.375Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['272','0','298','68','Untitled-1'],time:'2024-09-05T08:20:03.389Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['271','0','298','68','Untitled-1'],time:'2024-09-05T08:20:03.406Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['270','0','467','0','Untitled-1',"\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:03.408Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['270','0','297','31','Untitled-1'],time:'2024-09-05T08:20:03.422Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['269','0','467','0','Untitled-1',"    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:03.425Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['270','0','296','51','Untitled-1'],time:'2024-09-05T08:20:03.439Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['269','0','296','51','Untitled-1'],time:'2024-09-05T08:20:03.456Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['268','0','467','0','Untitled-1',"    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:03.458Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['269','0','295','35','Untitled-1'],time:'2024-09-05T08:20:03.472Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['268','0','295','35','Untitled-1'],time:'2024-09-05T08:20:03.489Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['267','0','467','0','Untitled-1',"    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:03.492Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['267','0','294','0','Untitled-1'],time:'2024-09-05T08:20:03.505Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['266','0','467','0','Untitled-1',"\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:03.509Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['267','0','293','92','Untitled-1'],time:'2024-09-05T08:20:03.522Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['266','0','293','92','Untitled-1'],time:'2024-09-05T08:20:03.539Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['265','0','467','0','Untitled-1',"    warning_data = json.loads(another_json_str)\n\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:03.542Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['266','0','292','96','Untitled-1'],time:'2024-09-05T08:20:03.556Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['265','0','292','96','Untitled-1'],time:'2024-09-05T08:20:03.572Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['264','0','467','0','Untitled-1',"for graph_id_str, another_json_str in code_content_data.items():\n    warning_data = json.loads(another_json_str)\n\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:03.575Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['264','0','291','89','Untitled-1'],time:'2024-09-05T08:20:03.589Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['263','0','467','0','Untitled-1',"\nfor graph_id_str, another_json_str in code_content_data.items():\n    warning_data = json.loads(another_json_str)\n\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:03.592Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['263','0','290','80','Untitled-1'],time:'2024-09-05T08:20:03.622Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['262','0','467','0','Untitled-1',"code_data = {}\n\nfor graph_id_str, another_json_str in code_content_data.items():\n    warning_data = json.loads(another_json_str)\n\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:03.625Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['263','0','289','0','Untitled-1'],time:'2024-09-05T08:20:03.639Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['263','0','467','0','Untitled-1',"\nfor graph_id_str, another_json_str in code_content_data.items():\n    warning_data = json.loads(another_json_str)\n\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:03.689Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['264','0','467','0','Untitled-1',"for graph_id_str, another_json_str in code_content_data.items():\n    warning_data = json.loads(another_json_str)\n\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:03.756Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['265','0','467','0','Untitled-1',"    warning_data = json.loads(another_json_str)\n\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:03.806Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['266','0','467','0','Untitled-1',"\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:03.873Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['265','0','467','0','Untitled-1',"    warning_data = json.loads(another_json_str)\n\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:04.323Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['264','0','467','0','Untitled-1',"for graph_id_str, another_json_str in code_content_data.items():\n    warning_data = json.loads(another_json_str)\n\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:04.356Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['263','0','467','0','Untitled-1',"\nfor graph_id_str, another_json_str in code_content_data.items():\n    warning_data = json.loads(another_json_str)\n\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:04.556Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['262','0','467','0','Untitled-1',"code_data = {}\n\nfor graph_id_str, another_json_str in code_content_data.items():\n    warning_data = json.loads(another_json_str)\n\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:04.724Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['262','0','289','0','Untitled-1'],time:'2024-09-05T08:20:04.725Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['261','0','467','0','Untitled-1',"\ncode_data = {}\n\nfor graph_id_str, another_json_str in code_content_data.items():\n    warning_data = json.loads(another_json_str)\n\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:04.828Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['261','0','288','1','Untitled-1'],time:'2024-09-05T08:20:04.839Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['260','0','467','0','Untitled-1',"    code_content_data = json.load(file)\n\ncode_data = {}\n\nfor graph_id_str, another_json_str in code_content_data.items():\n    warning_data = json.loads(another_json_str)\n\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:04.844Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['261','0','287','31','Untitled-1'],time:'2024-09-05T08:20:04.856Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['260','0','287','31','Untitled-1'],time:'2024-09-05T08:20:04.873Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['259','0','467','0','Untitled-1',"with open('meteor_app/private/original_graphs/' + warning_type + '_elementpositions.json', 'r') as file:\n    code_content_data = json.load(file)\n\ncode_data = {}\n\nfor graph_id_str, another_json_str in code_content_data.items():\n    warning_data = json.loads(another_json_str)\n\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:04.877Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['259','0','286','28','Untitled-1'],time:'2024-09-05T08:20:04.889Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['258','0','467','0','Untitled-1',"# Load code data from JSON\nwith open('meteor_app/private/original_graphs/' + warning_type + '_elementpositions.json', 'r') as file:\n    code_content_data = json.load(file)\n\ncode_data = {}\n\nfor graph_id_str, another_json_str in code_content_data.items():\n    warning_data = json.loads(another_json_str)\n\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:04.892Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['258','0','285','28','Untitled-1'],time:'2024-09-05T08:20:04.906Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['257','0','467','0','Untitled-1',"\n# Load code data from JSON\nwith open('meteor_app/private/original_graphs/' + warning_type + '_elementpositions.json', 'r') as file:\n    code_content_data = json.load(file)\n\ncode_data = {}\n\nfor graph_id_str, another_json_str in code_content_data.items():\n    warning_data = json.loads(another_json_str)\n\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:04.909Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['257','0','284','28','Untitled-1'],time:'2024-09-05T08:20:04.922Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['257','0','283','13','Untitled-1'],time:'2024-09-05T08:20:04.939Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['256','0','467','0','Untitled-1',"        graph_id_to_warning[graph_id] = example_id\n\n# Load code data from JSON\nwith open('meteor_app/private/original_graphs/' + warning_type + '_elementpositions.json', 'r') as file:\n    code_content_data = json.load(file)\n\ncode_data = {}\n\nfor graph_id_str, another_json_str in code_content_data.items():\n    warning_data = json.loads(another_json_str)\n\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:04.942Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['256','0','283','13','Untitled-1'],time:'2024-09-05T08:20:04.955Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['255','0','467','0','Untitled-1',"        example_id = line.split(',')[2].split(' - ')[0]\n        graph_id_to_warning[graph_id] = example_id\n\n# Load code data from JSON\nwith open('meteor_app/private/original_graphs/' + warning_type + '_elementpositions.json', 'r') as file:\n    code_content_data = json.load(file)\n\ncode_data = {}\n\nfor graph_id_str, another_json_str in code_content_data.items():\n    warning_data = json.loads(another_json_str)\n\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:04.958Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['255','0','282','82','Untitled-1'],time:'2024-09-05T08:20:04.972Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['254','0','467','0','Untitled-1',"        graph_id = line.split(',')[1]\n        example_id = line.split(',')[2].split(' - ')[0]\n        graph_id_to_warning[graph_id] = example_id\n\n# Load code data from JSON\nwith open('meteor_app/private/original_graphs/' + warning_type + '_elementpositions.json', 'r') as file:\n    code_content_data = json.load(file)\n\ncode_data = {}\n\nfor graph_id_str, another_json_str in code_content_data.items():\n    warning_data = json.loads(another_json_str)\n\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:04.975Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['254','0','281','0','Untitled-1'],time:'2024-09-05T08:20:04.989Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['253','0','467','0','Untitled-1',"    for line in file:\n        graph_id = line.split(',')[1]\n        example_id = line.split(',')[2].split(' - ')[0]\n        graph_id_to_warning[graph_id] = example_id\n\n# Load code data from JSON\nwith open('meteor_app/private/original_graphs/' + warning_type + '_elementpositions.json', 'r') as file:\n    code_content_data = json.load(file)\n\ncode_data = {}\n\nfor graph_id_str, another_json_str in code_content_data.items():\n    warning_data = json.loads(another_json_str)\n\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:04.992Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['253','0','280','59','Untitled-1'],time:'2024-09-05T08:20:05.006Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['252','0','467','0','Untitled-1',"with open('meteor_app/private/original_graphs/' + warning_type + '_graph_id_mapping.txt', 'r') as file:\n    for line in file:\n        graph_id = line.split(',')[1]\n        example_id = line.split(',')[2].split(' - ')[0]\n        graph_id_to_warning[graph_id] = example_id\n\n# Load code data from JSON\nwith open('meteor_app/private/original_graphs/' + warning_type + '_elementpositions.json', 'r') as file:\n    code_content_data = json.load(file)\n\ncode_data = {}\n\nfor graph_id_str, another_json_str in code_content_data.items():\n    warning_data = json.loads(another_json_str)\n\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:05.008Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['253','0','279','0','Untitled-1'],time:'2024-09-05T08:20:05.022Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['252','0','278','139','Untitled-1'],time:'2024-09-05T08:20:05.039Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['251','0','467','0','Untitled-1',"graph_id_to_warning = {}\nwith open('meteor_app/private/original_graphs/' + warning_type + '_graph_id_mapping.txt', 'r') as file:\n    for line in file:\n        graph_id = line.split(',')[1]\n        example_id = line.split(',')[2].split(' - ')[0]\n        graph_id_to_warning[graph_id] = example_id\n\n# Load code data from JSON\nwith open('meteor_app/private/original_graphs/' + warning_type + '_elementpositions.json', 'r') as file:\n    code_content_data = json.load(file)\n\ncode_data = {}\n\nfor graph_id_str, another_json_str in code_content_data.items():\n    warning_data = json.loads(another_json_str)\n\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:05.042Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['251','0','278','139','Untitled-1'],time:'2024-09-05T08:20:05.056Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['250','0','467','0','Untitled-1',"\ngraph_id_to_warning = {}\nwith open('meteor_app/private/original_graphs/' + warning_type + '_graph_id_mapping.txt', 'r') as file:\n    for line in file:\n        graph_id = line.split(',')[1]\n        example_id = line.split(',')[2].split(' - ')[0]\n        graph_id_to_warning[graph_id] = example_id\n\n# Load code data from JSON\nwith open('meteor_app/private/original_graphs/' + warning_type + '_elementpositions.json', 'r') as file:\n    code_content_data = json.load(file)\n\ncode_data = {}\n\nfor graph_id_str, another_json_str in code_content_data.items():\n    warning_data = json.loads(another_json_str)\n\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:05.058Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['250','0','277','61','Untitled-1'],time:'2024-09-05T08:20:05.072Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['249','0','467','0','Untitled-1',"ground_truth = read_ground_truth(ground_truth_file)\n\ngraph_id_to_warning = {}\nwith open('meteor_app/private/original_graphs/' + warning_type + '_graph_id_mapping.txt', 'r') as file:\n    for line in file:\n        graph_id = line.split(',')[1]\n        example_id = line.split(',')[2].split(' - ')[0]\n        graph_id_to_warning[graph_id] = example_id\n\n# Load code data from JSON\nwith open('meteor_app/private/original_graphs/' + warning_type + '_elementpositions.json', 'r') as file:\n    code_content_data = json.load(file)\n\ncode_data = {}\n\nfor graph_id_str, another_json_str in code_content_data.items():\n    warning_data = json.loads(another_json_str)\n\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:05.075Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['249','0','276','0','Untitled-1'],time:'2024-09-05T08:20:05.088Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['248','0','467','0','Untitled-1',"# Read ground_truth data\nground_truth = read_ground_truth(ground_truth_file)\n\ngraph_id_to_warning = {}\nwith open('meteor_app/private/original_graphs/' + warning_type + '_graph_id_mapping.txt', 'r') as file:\n    for line in file:\n        graph_id = line.split(',')[1]\n        example_id = line.split(',')[2].split(' - ')[0]\n        graph_id_to_warning[graph_id] = example_id\n\n# Load code data from JSON\nwith open('meteor_app/private/original_graphs/' + warning_type + '_elementpositions.json', 'r') as file:\n    code_content_data = json.load(file)\n\ncode_data = {}\n\nfor graph_id_str, another_json_str in code_content_data.items():\n    warning_data = json.loads(another_json_str)\n\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:05.091Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['249','0','275','58','Untitled-1'],time:'2024-09-05T08:20:05.106Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['249','0','467','0','Untitled-1',"ground_truth = read_ground_truth(ground_truth_file)\n\ngraph_id_to_warning = {}\nwith open('meteor_app/private/original_graphs/' + warning_type + '_graph_id_mapping.txt', 'r') as file:\n    for line in file:\n        graph_id = line.split(',')[1]\n        example_id = line.split(',')[2].split(' - ')[0]\n        graph_id_to_warning[graph_id] = example_id\n\n# Load code data from JSON\nwith open('meteor_app/private/original_graphs/' + warning_type + '_elementpositions.json', 'r') as file:\n    code_content_data = json.load(file)\n\ncode_data = {}\n\nfor graph_id_str, another_json_str in code_content_data.items():\n    warning_data = json.loads(another_json_str)\n\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:05.123Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['250','0','467','0','Untitled-1',"\ngraph_id_to_warning = {}\nwith open('meteor_app/private/original_graphs/' + warning_type + '_graph_id_mapping.txt', 'r') as file:\n    for line in file:\n        graph_id = line.split(',')[1]\n        example_id = line.split(',')[2].split(' - ')[0]\n        graph_id_to_warning[graph_id] = example_id\n\n# Load code data from JSON\nwith open('meteor_app/private/original_graphs/' + warning_type + '_elementpositions.json', 'r') as file:\n    code_content_data = json.load(file)\n\ncode_data = {}\n\nfor graph_id_str, another_json_str in code_content_data.items():\n    warning_data = json.loads(another_json_str)\n\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:05.140Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['251','0','467','0','Untitled-1',"graph_id_to_warning = {}\nwith open('meteor_app/private/original_graphs/' + warning_type + '_graph_id_mapping.txt', 'r') as file:\n    for line in file:\n        graph_id = line.split(',')[1]\n        example_id = line.split(',')[2].split(' - ')[0]\n        graph_id_to_warning[graph_id] = example_id\n\n# Load code data from JSON\nwith open('meteor_app/private/original_graphs/' + warning_type + '_elementpositions.json', 'r') as file:\n    code_content_data = json.load(file)\n\ncode_data = {}\n\nfor graph_id_str, another_json_str in code_content_data.items():\n    warning_data = json.loads(another_json_str)\n\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:05.273Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['252','0','467','0','Untitled-1',"with open('meteor_app/private/original_graphs/' + warning_type + '_graph_id_mapping.txt', 'r') as file:\n    for line in file:\n        graph_id = line.split(',')[1]\n        example_id = line.split(',')[2].split(' - ')[0]\n        graph_id_to_warning[graph_id] = example_id\n\n# Load code data from JSON\nwith open('meteor_app/private/original_graphs/' + warning_type + '_elementpositions.json', 'r') as file:\n    code_content_data = json.load(file)\n\ncode_data = {}\n\nfor graph_id_str, another_json_str in code_content_data.items():\n    warning_data = json.loads(another_json_str)\n\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:05.289Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['253','0','467','0','Untitled-1',"    for line in file:\n        graph_id = line.split(',')[1]\n        example_id = line.split(',')[2].split(' - ')[0]\n        graph_id_to_warning[graph_id] = example_id\n\n# Load code data from JSON\nwith open('meteor_app/private/original_graphs/' + warning_type + '_elementpositions.json', 'r') as file:\n    code_content_data = json.load(file)\n\ncode_data = {}\n\nfor graph_id_str, another_json_str in code_content_data.items():\n    warning_data = json.loads(another_json_str)\n\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:05.423Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['254','0','467','0','Untitled-1',"        graph_id = line.split(',')[1]\n        example_id = line.split(',')[2].split(' - ')[0]\n        graph_id_to_warning[graph_id] = example_id\n\n# Load code data from JSON\nwith open('meteor_app/private/original_graphs/' + warning_type + '_elementpositions.json', 'r') as file:\n    code_content_data = json.load(file)\n\ncode_data = {}\n\nfor graph_id_str, another_json_str in code_content_data.items():\n    warning_data = json.loads(another_json_str)\n\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:05.490Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['255','0','467','0','Untitled-1',"        example_id = line.split(',')[2].split(' - ')[0]\n        graph_id_to_warning[graph_id] = example_id\n\n# Load code data from JSON\nwith open('meteor_app/private/original_graphs/' + warning_type + '_elementpositions.json', 'r') as file:\n    code_content_data = json.load(file)\n\ncode_data = {}\n\nfor graph_id_str, another_json_str in code_content_data.items():\n    warning_data = json.loads(another_json_str)\n\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:05.526Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['256','0','467','0','Untitled-1',"        graph_id_to_warning[graph_id] = example_id\n\n# Load code data from JSON\nwith open('meteor_app/private/original_graphs/' + warning_type + '_elementpositions.json', 'r') as file:\n    code_content_data = json.load(file)\n\ncode_data = {}\n\nfor graph_id_str, another_json_str in code_content_data.items():\n    warning_data = json.loads(another_json_str)\n\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:05.539Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['258','0','467','0','Untitled-1',"# Load code data from JSON\nwith open('meteor_app/private/original_graphs/' + warning_type + '_elementpositions.json', 'r') as file:\n    code_content_data = json.load(file)\n\ncode_data = {}\n\nfor graph_id_str, another_json_str in code_content_data.items():\n    warning_data = json.loads(another_json_str)\n\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:05.556Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['259','0','467','0','Untitled-1',"with open('meteor_app/private/original_graphs/' + warning_type + '_elementpositions.json', 'r') as file:\n    code_content_data = json.load(file)\n\ncode_data = {}\n\nfor graph_id_str, another_json_str in code_content_data.items():\n    warning_data = json.loads(another_json_str)\n\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:05.589Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['260','0','467','0','Untitled-1',"    code_content_data = json.load(file)\n\ncode_data = {}\n\nfor graph_id_str, another_json_str in code_content_data.items():\n    warning_data = json.loads(another_json_str)\n\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:05.623Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['262','0','467','0','Untitled-1',"code_data = {}\n\nfor graph_id_str, another_json_str in code_content_data.items():\n    warning_data = json.loads(another_json_str)\n\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:05.690Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['263','0','467','0','Untitled-1',"\nfor graph_id_str, another_json_str in code_content_data.items():\n    warning_data = json.loads(another_json_str)\n\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:05.723Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['264','0','467','0','Untitled-1',"for graph_id_str, another_json_str in code_content_data.items():\n    warning_data = json.loads(another_json_str)\n\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:05.773Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['265','0','467','0','Untitled-1',"    warning_data = json.loads(another_json_str)\n\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:05.790Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['266','0','467','0','Untitled-1',"\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:05.839Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['267','0','467','0','Untitled-1',"    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:05.856Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['268','0','467','0','Untitled-1',"    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:05.906Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['269','0','467','0','Untitled-1',"    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:05.956Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['271','0','467','0','Untitled-1',"    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:06.039Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['272','0','467','0','Untitled-1',"\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:06.056Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['273','0','467','0','Untitled-1',"    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:06.089Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['274','0','467','0','Untitled-1',"\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:06.256Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['275','0','467','0','Untitled-1',"    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:06.356Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['276','0','467','0','Untitled-1',"\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:06.373Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['249','0','276','0','Untitled-1'],time:'2024-09-05T08:20:06.373Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['277','61','467','0','Untitled-1',"\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:06.762Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['250','0','277','61','Untitled-1'],time:'2024-09-05T08:20:06.772Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['278','139','467','0','Untitled-1',"\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:06.778Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['251','0','277','61','Untitled-1'],time:'2024-09-05T08:20:06.794Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['251','0','278','139','Untitled-1'],time:'2024-09-05T08:20:06.806Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['279','0','467','0','Untitled-1',"\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:06.809Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['252','0','279','0','Untitled-1'],time:'2024-09-05T08:20:06.823Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['253','0','279','0','Untitled-1'],time:'2024-09-05T08:20:06.840Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['280','59','467','0','Untitled-1',"\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:06.845Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['253','0','280','59','Untitled-1'],time:'2024-09-05T08:20:06.859Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['281','0','467','0','Untitled-1',"\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:06.862Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['254','0','281','0','Untitled-1'],time:'2024-09-05T08:20:06.875Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['282','82','467','0','Untitled-1',"\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:06.877Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['255','0','281','0','Untitled-1'],time:'2024-09-05T08:20:06.892Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['255','0','282','82','Untitled-1'],time:'2024-09-05T08:20:06.906Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['283','13','467','0','Untitled-1',"\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:06.908Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['256','0','283','13','Untitled-1'],time:'2024-09-05T08:20:06.922Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['284','28','467','0','Untitled-1',"\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:06.925Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['257','0','283','13','Untitled-1'],time:'2024-09-05T08:20:06.939Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['258','0','284','28','Untitled-1'],time:'2024-09-05T08:20:06.956Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['285','28','467','0','Untitled-1',"\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:06.958Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['258','0','285','28','Untitled-1'],time:'2024-09-05T08:20:06.972Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['286','28','467','0','Untitled-1',"\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:06.974Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['259','0','286','28','Untitled-1'],time:'2024-09-05T08:20:06.989Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['287','31','467','0','Untitled-1',"\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:06.991Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['260','0','286','28','Untitled-1'],time:'2024-09-05T08:20:07.006Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['260','0','287','31','Untitled-1'],time:'2024-09-05T08:20:07.022Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['288','1','467','0','Untitled-1',"\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:07.024Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['261','0','288','1','Untitled-1'],time:'2024-09-05T08:20:07.039Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['289','0','467','0','Untitled-1',"\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:07.041Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['262','0','288','1','Untitled-1'],time:'2024-09-05T08:20:07.056Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['262','0','289','0','Untitled-1'],time:'2024-09-05T08:20:07.072Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['290','80','467','0','Untitled-1',"\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:07.075Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['290','0','467','0','Untitled-1',"# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:07.089Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['263','0','290','80','Untitled-1'],time:'2024-09-05T08:20:07.090Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['288','0','467','0','Untitled-1',"}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:07.173Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['287','0','467','0','Untitled-1',"    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:07.189Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['286','0','467','0','Untitled-1',"    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:07.206Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['285','0','467','0','Untitled-1',"    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:07.222Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['284','0','467','0','Untitled-1',"    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:07.306Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['283','0','467','0','Untitled-1',"scenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:07.339Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['282','0','467','0','Untitled-1',"# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:07.590Z'},{src:'onDidChangeActiveTextEditor',msg:'Current editor: [%s]; Previous editor: [%s]',prm:['','untitled:Untitled-1'],time:'2024-09-05T08:20:10.672Z'},{src:'onDidChangeActiveTextEditor',msg:'Current editor: [%s]; Previous editor: [%s]',prm:['/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',''],time:'2024-09-05T08:20:10.703Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['0','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"# the idea is to try different combinations \n# IMPORTANT: prequisites for running this script : run `meteor` in the `meteor_app` directory \n# Run one of \n# e.g., WARNING_TYPE=apache_lucene-solr__NULL_ WARNING_JSON_NAME=spotbugs_warnings_apache_lucene-solr__NULL_ meteor  \n# e.g., WARNING_JSON_NAME=infer_warnings_alibaba_nacos_NULL_DEREFERENCE meteor \n# e.g,, WARNING_TYPE=RESOURCE_LEAK__presto WARNING_JSON_NAME=infer_warnings_prestodb_presto_RESOURCE_LEAK meteor \n# and generate the background.lp file \n\nimport random\nimport subprocess\nimport sys\nimport os\nfrom collections import defaultdict\nimport json\nimport time\nimport matplotlib.pyplot as plt\n\ndef read_ground_truth(ground_truth_file):\n    ground_truth = {}\n    with open(ground_truth_file) as f:\n        for line in f:\n            line = line.strip().split()\n            ground_truth[line[0]] = line[1]\n    return ground_truth\n\ndef write_labels_to_clingo_input(poss, negs):\n    with open('lp/simulation_labels.lp', 'w+') as f:\n        for p in poss:\n            f.write(f'pos({p}).\\n')\n        for n in negs:\n            f.write(f'neg({n}).\\n')\n\ndef read_containment(containment_file='lp/background.lp'):\n    containment = {}\n    last_checked_id = -1  # Initialize last_checked_id to keep track of the last ID we processed\n\n    with open(containment_file) as f:\n        for line in f:\n            line = line.strip().split()\n\n            # If line contains 'containment', extract the warning ID\n            if 'containment' in line[0]:\n                id = int(line[0].split('containment(')[1].split(',')[0])\n\n                # Only process the first occurrence of each ID\n                if id != last_checked_id:\n                    loc = line[1].split(\")\")[0]\n                    loc = loc.replace('__', '.').replace('_', '.')\n                    last_checked_id = id  # Update last_checked_id to prevent re-checking this ID\n                    containment[id] = loc  # Store the location for this ID\n    return containment\n\ndef heuristic_shorter_code_first(warnings):\n    \"\"\"Sort the warnings by lines of code (ascending).\"\"\"\n    return sorted(warnings, key=lambda x: x[1])\n\ndef heuristic_shared_function_calls(warnings):\n    \"\"\"Sort the warnings by the number of shared function calls (descending).\"\"\"\n    def shared_function_calls(warning, other_warnings):\n        return sum(len(set(warning[2]).intersection(set(other[2]))) for other in other_warnings)\n    #print(sorted(warnings, key=lambda x: shared_function_calls(x, warnings), reverse=True))\n    return sorted(warnings, key=lambda x: shared_function_calls(x, warnings), reverse=True)\n\ndef heuristic_neighbor_classes(warnings, containment):\n    \"\"\"Sort the warnings by neighbor classes (contained in the same package or directory).\"\"\"\n    sorted_warnings = []\n    processed_locs = set()\n    \n    for i in range(len(warnings)):\n        loc = containment.get(int(warnings[i][0]))\n        if loc not in processed_locs:\n            # Find all warnings with the same location\n            same_loc_warnings = [warnings[j] for j in range(i, len(warnings)) if containment.get(int(warnings[j][0])) == loc]\n            sorted_warnings.extend(same_loc_warnings)\n            processed_locs.add(loc)\n    \n    return sorted_warnings\n\ndef initialize_warnings_state(ground_truth):\n    \"\"\"Initialize all warnings with the state 'uninspected'.\"\"\"\n    return {k: 'uninspected' for k in ground_truth.keys()}\n\ndef calculate_accuracy(warnings_state, ground_truth):\n    \"\"\"Calculate the accuracy by comparing the current state of warnings with the ground truth.\"\"\"\n    correct_labels = sum(1 for k, v in warnings_state.items() if v == ground_truth[k])\n    return correct_labels / len(ground_truth) * 100\n\ndef sample_labels_randomized_then_sorted(ground_truth, num_pos, num_neg, code_data, warnings_state, apply_heuristics=None):\n    \"\"\"\n    Apply specified heuristics to the warnings before sampling:\n    - apply_heuristics: A list containing the numbers [1, 2, 3] corresponding to the heuristics to apply.\n        1: Review the shorter code first.\n        2: Look for similar code (shared API calls).\n        3: Look for neighbor classes (contained in the same package or directory).\n    - After applying heuristics, select either one positive or one negative warning based on a coin toss.\n    \"\"\"\n    if apply_heuristics is None:\n        apply_heuristics = [1, 2, 3]  # Default to applying all heuristics\n    \n    # Get all uninspected warnings with their lines of code and function calls\n    warnings = [\n        (k, code_data.get(int(k), {}).get('linesOfCode', 0), code_data.get(int(k), {}).get('functionCalls', []))\n        for k, v in ground_truth.items() if warnings_state[k] == 'uninspected'\n    ]\n    \n    if not warnings:  # If all warnings have been inspected, return empty lists\n        return [], []\n    \n    # Apply heuristics in the specified order\n    containment = read_containment()\n    for heuristic in apply_heuristics:\n        if heuristic == 1:\n            warnings = heuristic_shorter_code_first(warnings)\n        elif heuristic == 2:\n            warnings = heuristic_shared_function_calls(warnings)\n        elif heuristic == 3:\n            warnings = heuristic_neighbor_classes(warnings, containment)\n\n    # Sort by code length to identify shortest and longest warnings\n    sorted_warnings_by_length = sorted(warnings, key=lambda x: x[1])\n    \n    # Coin toss to decide whether to pick a positive or negative warning\n    coin_toss = random.choice(['positive', 'negative'])\n    \n    if coin_toss == 'positive':\n        # Select the shortest warning for positive\n        selected_positive_warnings = [sorted_warnings_by_length[0][0]]\n        selected_negative_warnings = []\n        warnings_state[selected_positive_warnings[0]] = 'positive'\n        #print('Coin toss result: Positive selected')\n    else:\n        # Select the longest warning for negative\n        selected_positive_warnings = []\n        selected_negative_warnings = [sorted_warnings_by_length[-1][0]]\n        warnings_state[selected_negative_warnings[0]] = 'negative'\n        #print('Coin toss result: Negative selected')\n    \n    print('Selected positive warning:', selected_positive_warnings)\n    print('Selected negative warning:', selected_negative_warnings)\n    \n    return selected_positive_warnings, selected_negative_warnings\n\n\ndef run_clingo():\n    files = [\n        f'lp/simulation_labels.lp',  \n        'lp/background.lp',              \n        'lp/frozen_rules.lp',            \n        'lp/rules2.lp'                   \n    ]\n    \n    command = ['clingo'] + files + ['--outf=2'] + ['--time-limit=30'] \n    print(' '.join(command))\n    result = subprocess.run(command, capture_output=True, text=True)\n\n    return result.stdout\n\ndef parse_clingo_output(data):\n    # Load the data using json.loads if 'data' is a string,\n    # otherwise assume it's already a dictionary\n    if isinstance(data, str):\n        data = json.loads(data)\n    \n    # Navigate through the JSON structure\n    # Assuming 'Call' is always present and has at least one element\n    calls = data.get(\"Call\", [])\n    if calls:\n        # Assuming 'Witnesses' is always present in the last element of 'Call' and has at least one element\n        last_call = calls[-1]\n        witnesses = last_call.get(\"Witnesses\", [])\n        if witnesses:\n            # Get the last 'Witnesses' entry\n            last_witness = witnesses[-1]\n            # Return the 'Value' list from the last 'Witnesses' entry\n            return last_witness.get(\"Value\", [])\n    return []\n\ndef extract_summary_rules(clingo_output):\n    # a summary rule is one prefixed by rule_contains(number)\n    summary_rules_by_prefix = defaultdict(list)\n    for line in clingo_output:\n        if line.startswith('rule_contains'):\n\n            number_str = line.split('(')[0].split('rule_contains')[1]\n            number = int(number_str) if number_str.isdigit() else 0\n\n            rule = line.split('(')[1].split(')')[0]\n            summary_rules_by_prefix[number].append(rule)\n    return summary_rules_by_prefix\n\ndef calculate_rule_percentage(clingo_output, positive_predictions):\n    summary_rules_by_prefix = defaultdict(list)\n    rule_percentages = {}\n    for line in clingo_output:\n        if line.startswith('rule_predict_pos'):\n            number_str = line.split('(')[0].split('rule_predict_pos')[1]\n            if number_str == '':\n                number_str = '0'\n            number = int(number_str)\n            warning_number = line.split('(')[1].split(')')[0]\n            summary_rules_by_prefix[number].append(warning_number)\n    for rule_number, matched_warnings in summary_rules_by_prefix.items():\n        rule_percentages[rule_number] = len(set(matched_warnings) & set(positive_predictions)) / len(set(matched_warnings))\n    return rule_percentages\n\ndef number_of_rules_over_percentage(percentages, percentage_threshold=0.8):\n    return sum(1 for p in percentages.values() if p >= percentage_threshold)\n\ndef get_number_of_positive_predictions(clingo_output):\n    positive_predictions = set()\n    for line in clingo_output:\n        if line.startswith('rule_predict_pos'):\n            warning_number = line.split('(')[1].split(')')[0]\n            positive_predictions.add(warning_number)\n    return len(positive_predictions)\n\n\ndef get_positive_predictions(clingo_output, rule_numbers):\n    # rule_predict_pos<number>(<warning>)\n    positive_predictions = []\n    for line in clingo_output:\n        if line.startswith('rule_predict_pos'):\n            number_str = line.split('(')[0].split('rule_predict_pos')[1]\n            number = int(number_str) if number_str.isdigit() else 0\n            if number not in rule_numbers:\n                continue\n            # extract warning from parenthesis\n            warning = line.split('(')[1].split(')')[0]\n            positive_predictions.append(warning)\n    return positive_predictions\n\ndef get_positive_predictions_of_rule(clingo_output, rule_number):\n    # rule_predict_pos<number>(<warning>)\n    positive_predictions = []\n    for line in clingo_output:\n        matches_rule = line.startswith(f'rule_predict_pos{rule_number}') if rule_number != 0 else line.startswith('rule_predict_pos')\n        if matches_rule:\n            # extract warning from parenthesis\n            warning = line.split('(')[1].split(')')[0]\n            positive_predictions.append(warning)\n    return positive_predictions\n\n# Simulation driver code\n\n# Parameters for the simulation\nwarning_type = sys.argv[1]\nground_truth_file = sys.argv[2]\n\n# Read ground_truth data\nground_truth = read_ground_truth(ground_truth_file)\n\ngraph_id_to_warning = {}\nwith open('meteor_app/private/original_graphs/' + warning_type + '_graph_id_mapping.txt', 'r') as file:\n    for line in file:\n        graph_id = line.split(',')[1]\n        example_id = line.split(',')[2].split(' - ')[0]\n        graph_id_to_warning[graph_id] = example_id\n\n# Load code data from JSON\nwith open('meteor_app/private/original_graphs/' + warning_type + '_elementpositions.json', 'r') as file:\n    code_content_data = json.load(file)\n\ncode_data = {}\n\nfor graph_id_str, another_json_str in code_content_data.items():\n    warning_data = json.loads(another_json_str)\n\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n    try:\n        warning_id = int(graph_id_to_warning[graph_id_str])\n    except:\n        continue\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:10.709Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['398','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:10.709Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['415','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:10.709Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['423','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',''],time:'2024-09-05T08:20:11.265Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['422','10','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py','\n'],time:'2024-09-05T08:20:11.691Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['421','14','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py','\nplt.show()\n'],time:'2024-09-05T08:20:11.708Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['420','12','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py','\nplt.grid(True)\nplt.show()\n'],time:'2024-09-05T08:20:11.724Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['417','11','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"'Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:11.741Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['415','6','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"  plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:11.757Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['414','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:11.775Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['414','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:11.789Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['413','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:11.792Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['412','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:11.806Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['411','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:11.809Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['410','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:11.823Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['409','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"plt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:11.825Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['408','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:11.839Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['407','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"plt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:11.842Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['406','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:11.856Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['405','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"plt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:11.858Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['404','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:11.873Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['403','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:11.875Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['402','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:11.889Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['401','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:11.893Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['400','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:11.906Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['399','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"plt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:11.909Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['398','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:11.922Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['397','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:11.925Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['396','0','422','10','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:11.939Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['395','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"plt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:11.941Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['394','0','420','12','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:11.956Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['393','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"plt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:11.959Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['392','0','418','25','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:11.973Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['391','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"plt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:11.975Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['390','0','416','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:11.989Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['389','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:11.992Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['388','0','413','60','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:12.006Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['387','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:12.010Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['386','0','411','68','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:12.023Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['385','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:12.026Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['384','0','409','10','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:12.041Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['383','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:12.044Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['383','0','408','14','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:12.060Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['382','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:12.062Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['385','1','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'," Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:12.075Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['389','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:12.089Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['391','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"plt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:12.105Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['392','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"plt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:12.139Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['393','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"plt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:12.189Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['394','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"plt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:12.223Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['393','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"plt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:12.373Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['392','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"plt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:12.406Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['391','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"plt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:12.473Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['390','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:12.490Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['389','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:12.506Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['388','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:12.540Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['387','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:12.589Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['386','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"plt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:12.639Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['385','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:12.856Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['384','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:12.940Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['383','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:13.090Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['382','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:13.223Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['382','0','408','14','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:13.224Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['381','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:13.362Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['382','0','407','12','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:13.372Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['381','0','407','12','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:13.389Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['380','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:13.393Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['381','0','406','105','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:13.405Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['380','0','406','105','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:13.422Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['379','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:13.425Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['380','0','405','52','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:13.439Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['379','0','405','52','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:13.456Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['378','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:13.459Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['378','0','404','30','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:13.472Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['378','0','403','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:13.489Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['377','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:13.492Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['377','0','403','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:13.505Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['376','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:13.509Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['377','0','402','122','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:13.522Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['376','0','401','59','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:13.539Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['375','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:13.542Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['375','0','401','59','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:13.555Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['374','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:13.558Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['375','0','400','64','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:13.572Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['374','0','400','64','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:13.589Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['373','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:13.591Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['374','0','399','27','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:13.605Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['373','0','398','91','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:13.622Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['372','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:13.625Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['372','0','398','91','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:13.639Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['371','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:13.642Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['372','0','397','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:13.655Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['371','0','397','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:13.672Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['370','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:13.675Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['371','0','396','10','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:13.689Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['370','0','395','14','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:13.705Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['369','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:13.708Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['369','0','395','14','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:13.722Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['368','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:13.726Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['369','0','394','12','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:13.738Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['368','0','394','12','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:13.756Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['367','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:13.758Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['367','0','393','79','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:13.772Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['367','0','392','26','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:13.789Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['366','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:13.792Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['366','0','392','26','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:13.805Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['365','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:13.808Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['366','0','391','30','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:13.822Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['365','0','391','30','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:13.838Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['364','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:13.841Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['364','0','390','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:13.855Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['364','0','389','108','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:13.872Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['363','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:13.875Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['363','0','389','108','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:13.889Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['362','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:13.891Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['363','0','388','52','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:13.906Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['362','0','387','57','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:13.922Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['361','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:13.925Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['361','0','387','57','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:13.939Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['360','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:13.942Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['361','0','386','27','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:13.955Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['360','0','386','27','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:13.972Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['359','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:13.975Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['360','0','385','65','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:13.989Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['359','0','384','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.005Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['358','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:14.008Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['358','0','384','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.022Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['357','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:14.025Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['358','0','383','80','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.039Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['357','0','383','80','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.055Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['356','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:14.058Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['357','0','382','108','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.072Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['356','0','381','75','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.089Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['355','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:14.092Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['355','0','381','75','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.105Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['354','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:14.108Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['355','0','380','76','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.122Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['354','0','380','76','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.139Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['353','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:14.142Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['353','0','379','71','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.155Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['353','0','378','53','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.172Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['352','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:14.175Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['352','0','378','53','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.189Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['351','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:14.192Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['352','0','377','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.206Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['351','0','376','86','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.222Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['350','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:14.225Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['350','0','376','86','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.239Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['349','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:14.241Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['350','0','375','94','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.255Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['349','0','375','94','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.272Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['348','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:14.275Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['349','0','374','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.289Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['348','0','373','39','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.305Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['347','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:14.308Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['347','0','373','39','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.322Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['346','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:14.325Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['347','0','372','21','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.339Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['346','0','372','21','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.355Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['345','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:14.358Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['346','0','371','82','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.372Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['345','0','370','33','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.389Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['344','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:14.392Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['344','0','370','33','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.406Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['343','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:14.409Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['344','0','369','70','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.422Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['343','0','369','70','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.438Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['342','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:14.442Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['342','0','368','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.455Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['342','0','367','35','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.472Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['341','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:14.476Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['341','0','367','35','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.489Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['340','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:14.492Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['341','0','366','21','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.505Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['340','0','365','91','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.522Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['339','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:14.526Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['339','0','365','91','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.539Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['338','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:14.542Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['339','0','364','48','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.555Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['338','0','364','48','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.572Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['337','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:14.575Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['338','0','363','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.590Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['337','0','362','90','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.606Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['336','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:14.609Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['336','0','362','90','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.627Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['335','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:14.631Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['336','0','361','47','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.640Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['335','0','361','47','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.655Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['334','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:14.658Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['335','0','360','87','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.672Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['334','0','359','74','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.689Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['333','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:14.692Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['333','0','359','74','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.705Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['332','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:14.708Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['333','0','358','61','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.722Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['332','0','358','61','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.739Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['331','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:14.742Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['331','0','357','51','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.756Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['331','0','356','22','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.772Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['330','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:14.775Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['330','0','356','22','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.789Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['329','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:14.792Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['330','0','355','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.805Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['329','0','354','33','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.822Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['328','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:14.825Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['328','0','354','33','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.839Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['327','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:14.842Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['328','0','353','64','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.855Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['327','0','353','64','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.872Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['326','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:14.875Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['327','0','352','82','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.889Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['326','0','351','82','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.905Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['325','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:14.908Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['325','0','351','82','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.922Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['324','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:14.925Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['325','0','350','57','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.939Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['324','0','350','57','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.955Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['323','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:14.958Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['324','0','349','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.972Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['323','0','348','157','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:14.989Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['322','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:14.992Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['322','0','348','157','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.005Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:15.008Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['322','0','347','69','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.022Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['321','0','347','69','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.039Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['320','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:15.042Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['320','0','346','17','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.055Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['320','0','345','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.072Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['319','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:15.076Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['319','0','345','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.089Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['318','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:15.092Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['319','0','344','161','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.105Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['318','0','343','21','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.122Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['317','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:15.126Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['317','0','343','21','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.139Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['316','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:15.142Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['317','0','342','64','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.155Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['316','0','342','64','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.172Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['315','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:15.175Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['316','0','341','52','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.189Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['315','0','340','43','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.206Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['314','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:15.209Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['314','0','340','43','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.222Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['313','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:15.225Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['314','0','339','92','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.239Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['313','0','339','92','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.255Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['312','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:15.259Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['313','0','338','53','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.272Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['312','0','337','96','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.289Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['311','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:15.292Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['311','0','337','96','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.306Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['310','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:15.309Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['311','0','336','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.321Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['310','0','336','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.339Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['309','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:15.342Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['309','0','335','89','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.355Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['309','0','334','111','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.372Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['308','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:15.375Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['308','0','334','111','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.389Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['307','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:15.392Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['308','0','333','40','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.405Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['307','0','332','80','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.422Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['306','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:15.427Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['306','0','332','80','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.439Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['305','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:15.442Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['306','0','331','59','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.455Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['305','0','331','59','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.471Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['304','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:15.474Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['305','0','330','30','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.488Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['304','0','329','41','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.505Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['303','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:15.507Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['303','0','329','41','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.521Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['302','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:15.524Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['303','0','328','41','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.538Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['302','0','328','41','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.555Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['301','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:15.557Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['302','0','327','86','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.571Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['301','0','326','32','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.588Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['300','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:15.591Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['300','0','326','32','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.605Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['299','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:15.607Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['300','0','325','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.621Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['299','0','325','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.638Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['298','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:15.641Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['298','0','324','60','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.654Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['298','0','323','48','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.671Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['297','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:15.674Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['297','0','323','48','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.688Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['296','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:15.690Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['297','0','322','39','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.704Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['296','0','321','88','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.721Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['295','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"conciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:15.725Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['295','0','321','88','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.738Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['294','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"rule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:15.741Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['295','0','320','49','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.754Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['294','0','320','49','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.771Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['293','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"accuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:15.774Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['294','0','319','92','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.788Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['293','0','318','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.805Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['292','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:15.807Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['292','0','318','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.822Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['291','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:15.825Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['292','0','317','85','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.840Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['291','0','317','85','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.861Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['290','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:15.861Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['290','0','316','107','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.878Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['289','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:15.883Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['290','0','315','36','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.889Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['289','0','315','36','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.906Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['288','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:15.909Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['289','0','314','76','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.922Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['288','0','314','76','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.939Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['287','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:15.941Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['287','0','313','55','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.955Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['287','0','312','26','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.972Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['286','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:15.975Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['286','0','312','26','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:15.989Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['285','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"scenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:15.992Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['286','0','311','37','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:16.005Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['285','0','310','68','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:16.022Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['284','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:16.025Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['284','0','310','68','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:16.039Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['283','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:16.042Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['284','0','309','28','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:16.055Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['283','0','309','28','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:16.072Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['282','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:16.075Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['283','0','308','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:16.089Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['282','0','307','91','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:16.105Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['281','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:16.108Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['281','0','307','91','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:16.122Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['280','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:16.125Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['281','0','306','45','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:16.139Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['280','0','306','45','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:16.155Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['279','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:16.158Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['280','0','305','53','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:16.172Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['279','0','304','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:16.189Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['278','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:16.191Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['278','0','304','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:16.206Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['277','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:16.208Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['278','0','303','40','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:16.222Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['277','0','303','40','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:16.239Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['276','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:16.241Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['276','0','302','64','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:16.255Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['276','0','301','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:16.272Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['275','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:16.275Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['275','0','301','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:16.289Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['274','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:16.292Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['275','0','300','68','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:16.305Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['274','0','299','31','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:16.322Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['273','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"        continue\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:16.325Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['273','0','299','31','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:16.339Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['272','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"    except:\n        continue\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:16.341Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['273','0','298','51','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:16.356Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['273','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"        continue\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:16.406Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['274','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:16.489Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['275','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:16.523Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['276','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:16.556Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['277','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:16.589Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['278','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:18.177Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['279','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:18.257Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['280','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:18.290Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['281','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:18.373Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['282','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:18.440Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['283','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:18.556Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['284','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:18.758Z'},{src:'onDidChangeTextDocument',msg:'%s:%s to %s:%s in [%s] replaced with: %s`',prm:['284','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:21.346Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['469','0','469','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',''],time:'2024-09-05T08:20:21.349Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['444','0','469','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:21.350Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['436','0','461','9','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:22.256Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['427','0','453','55','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:22.485Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['419','0','445','105','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:22.857Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['411','0','436','34','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:22.914Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['402','0','428','10','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:23.200Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['394','0','420','22','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:23.215Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['386','0','411','27','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:23.239Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['372','0','397','19','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:23.289Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['364','0','389','49','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:23.496Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['355','0','381','76','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:23.512Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['344','0','370','33','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:23.525Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['330','0','356','22','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:23.539Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['314','0','339','92','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:23.589Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['305','0','331','59','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:23.756Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['297','0','322','39','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:23.789Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['286','0','311','37','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:23.806Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['272','0','297','35','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:23.822Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['255','0','281','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:23.839Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['247','0','272','11','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:24.006Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['239','0','264','64','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:24.018Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['227','0','253','21','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:24.032Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['214','0','239','48','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:24.048Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['197','0','222','72','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:24.072Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['189','0','214','36','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:24.239Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['180','0','206','76','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:24.256Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['169','0','195','72','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:24.272Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['155','0','181','44','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:24.289Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['139','0','164','68','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:24.306Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['130','0','156','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:24.467Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['122','0','147','41','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:24.489Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['111','0','136','4','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:24.507Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['97','0','122','55','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:24.524Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['80','0','106','21','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:24.541Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['72','0','97','74','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:24.706Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['64','0','89','63','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:24.722Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['52','0','78','44','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:24.739Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['39','0','64','93','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:24.756Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['22','0','47','66','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:24.772Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['14','0','39','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:24.933Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['5','0','31','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:24.956Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['0','0','26','52','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:20:24.972Z'},{src:'onDidChangeActiveTerminal',msg:'Current terminal: [%s]; Previous terminal: [%s]',prm:['zsh','zsh'],time:'2024-09-05T08:20:34.508Z'}]
[{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['189','33','189','33','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',''],time:'2024-09-06T20:57:14.916Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['159','0','190','56','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T20:57:14.916Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['162','0','193','67','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T20:57:14.917Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['170','18','170','18','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',''],time:'2024-09-06T20:57:15.293Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['0','0','417','86','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"# the idea is to try different combinations \n# IMPORTANT: prequisites for running this script : run `meteor` in the `meteor_app` directory \n# Run one of \n# e.g., WARNING_TYPE=apache_lucene-solr__NULL_ WARNING_JSON_NAME=spotbugs_warnings_apache_lucene-solr__NULL_ meteor  \n# e.g., WARNING_JSON_NAME=infer_warnings_alibaba_nacos_NULL_DEREFERENCE meteor \n# e.g,, WARNING_TYPE=RESOURCE_LEAK__presto WARNING_JSON_NAME=infer_warnings_prestodb_presto_RESOURCE_LEAK meteor \n# and generate the background.lp file \n\nimport random\nimport subprocess\nimport sys\nimport os\nfrom collections import defaultdict\nimport json\nimport time\nimport matplotlib.pyplot as plt\n\ndef read_ground_truth(ground_truth_file):\n    ground_truth = {}\n    with open(ground_truth_file) as f:\n        for line in f:\n            line = line.strip().split()\n            ground_truth[line[0]] = line[1]\n    return ground_truth\n\ndef write_labels_to_clingo_input(poss, negs):\n    with open('lp/simulation_labels.lp', 'w+') as f:\n        for p in poss:\n            f.write(f'pos({p}).\\n')\n        for n in negs:\n            f.write(f'neg({n}).\\n')\n\ndef read_containment(containment_file='lp/background.lp'):\n    containment = {}\n    last_checked_id = -1  # Initialize last_checked_id to keep track of the last ID we processed\n\n    with open(containment_file) as f:\n        for line in f:\n            line = line.strip().split()\n\n            # If line contains 'containment', extract the warning ID\n            if 'containment' in line[0]:\n                id = int(line[0].split('containment(')[1].split(',')[0])\n\n                # Only process the first occurrence of each ID\n                if id != last_checked_id:\n                    loc = line[1].split(\")\")[0]\n                    loc = loc.replace('__', '.').replace('_', '.')\n                    last_checked_id = id  # Update last_checked_id to prevent re-checking this ID\n                    containment[id] = loc  # Store the location for this ID\n    return containment\n\ndef heuristic_shorter_code_first(warnings):\n    \"\"\"Sort the warnings by lines of code (ascending).\"\"\"\n    return sorted(warnings, key=lambda x: x[1])\n\ndef heuristic_shared_function_calls(warnings):\n    \"\"\"Sort the warnings by the number of shared function calls (descending).\"\"\"\n    def shared_function_calls(warning, other_warnings):\n        return sum(len(set(warning[2]).intersection(set(other[2]))) for other in other_warnings)\n    #print(sorted(warnings, key=lambda x: shared_function_calls(x, warnings), reverse=True))\n    return sorted(warnings, key=lambda x: shared_function_calls(x, warnings), reverse=True)\n\ndef heuristic_neighbor_classes(warnings, containment):\n    \"\"\"Sort the warnings by neighbor classes (contained in the same package or directory).\"\"\"\n    sorted_warnings = []\n    processed_locs = set()\n    \n    for i in range(len(warnings)):\n        loc = containment.get(int(warnings[i][0]))\n        if loc not in processed_locs:\n            # Find all warnings with the same location\n            same_loc_warnings = [warnings[j] for j in range(i, len(warnings)) if containment.get(int(warnings[j][0])) == loc]\n            sorted_warnings.extend(same_loc_warnings)\n            processed_locs.add(loc)\n    \n    return sorted_warnings\n\ndef initialize_warnings_state(ground_truth):\n    \"\"\"Initialize all warnings with the state 'uninspected'.\"\"\"\n    return {k: 'uninspected' for k in ground_truth.keys()}\n\ndef calculate_accuracy(warnings_state, ground_truth):\n    \"\"\"Calculate the accuracy by comparing the current state of warnings with the ground truth.\"\"\"\n    correct_labels = sum(1 for k, v in warnings_state.items() if v == ground_truth[k])\n    return correct_labels / len(ground_truth) * 100\n\ndef sample_labels_randomized_then_sorted(ground_truth, num_pos, num_neg, code_data, warnings_state, apply_heuristics=None):\n    \"\"\"\n    Apply specified heuristics to the warnings before sampling:\n    - apply_heuristics: A list containing the numbers [1, 2, 3] corresponding to the heuristics to apply.\n        1: Review the shorter code first.\n        2: Look for similar code (shared API calls).\n        3: Look for neighbor classes (contained in the same package or directory).\n    - After applying heuristics, select either one positive or one negative warning based on a coin toss.\n    \"\"\"\n    if apply_heuristics is None:\n        apply_heuristics = [1, 2, 3]  # Default to applying all heuristics\n    \n    # Get all uninspected warnings with their lines of code and function calls\n    warnings = [\n        (k, code_data.get(int(k), {}).get('linesOfCode', 0), code_data.get(int(k), {}).get('functionCalls', []))\n        for k, v in ground_truth.items() if warnings_state[k] == 'uninspected'\n    ]\n    \n    if not warnings:  # If all warnings have been inspected, return empty lists\n        return [], []\n    \n    # Apply heuristics in the specified order\n    containment = read_containment()\n    for heuristic in apply_heuristics:\n        if heuristic == 1:\n            warnings = heuristic_shorter_code_first(warnings)\n        elif heuristic == 2:\n            warnings = heuristic_shared_function_calls(warnings)\n        elif heuristic == 3:\n            warnings = heuristic_neighbor_classes(warnings, containment)\n        elif heuristic == 4:\n            # choose a random heuristic\n            warnings = random.choice([heuristic_shorter_code_first(warnings), heuristic_shared_function_calls(warnings), heuristic_neighbor_classes(warnings, containment)])\n\n    # Sort by code length to identify shortest and longest warnings\n    sorted_warnings_by_length = sorted(warnings, key=lambda x: x[1])\n    \n    # Coin toss to decide whether to pick a positive or negative warning\n    coin_toss = random.choice(['positive', 'negative'])\n    \n    if coin_toss == 'positive':\n        # Select the shortest warning for positive\n        selected_positive_warnings = [sorted_warnings_by_length[0][0]]\n        selected_negative_warnings = []\n        warnings_state[selected_positive_warnings[0]] = 'positive'\n        #print('Coin toss result: Positive selected')\n    else:\n        # Select the longest warning for negative\n        selected_positive_warnings = []\n        selected_negative_warnings = [sorted_warnings_by_length[-1][0]]\n        warnings_state[selected_negative_warnings[0]] = 'negative'\n        #print('Coin toss result: Negative selected')\n    \n    print('Selected positive warning:', selected_positive_warnings)\n    print('Selected negative warning:', selected_negative_warnings)\n    \n    return selected_positive_warnings, selected_negative_warnings\n\n\ndef run_clingo():\n    files = [\n        f'lp/simulation_labels.lp',  \n        'lp/background.lp',              \n        'lp/frozen_rules.lp',            \n        'lp/rules2.lp'                   \n    ]\n    \n    command = ['clingo'] + files + ['--outf=2'] + ['--time-limit=30'] \n    print(' '.join(command))\n    result = subprocess.run(command, capture_output=True, text=True)\n\n    return result.stdout\n\ndef parse_clingo_output(data):\n    # Load the data using json.loads if 'data' is a string,\n    # otherwise assume it's already a dictionary\n    if isinstance(data, str):\n        print(data)\n        data = json.loads(data)\n    # Navigate through the JSON structure\n    # Assuming 'Call' is always present and has at least one element\n    calls = data.get(\"Call\", [])\n    if calls:\n        # Assuming 'Witnesses' is always present in the last element of 'Call' and has at least one element\n        last_call = calls[-1]\n        witnesses = last_call.get(\"Witnesses\", [])\n        if witnesses:\n            # Get the last 'Witnesses' entry\n            last_witness = witnesses[-1]\n            # Return the 'Value' list from the last 'Witnesses' entry\n            return last_witness.get(\"Value\", [])\n    return []\n\ndef extract_summary_rules(clingo_output):\n    # a summary rule is one prefixed by rule_contains(number)\n    summary_rules_by_prefix = defaultdict(list)\n    for line in clingo_output:\n        if line.startswith('rule_contains'):\n\n            number_str = line.split('(')[0].split('rule_contains')[1]\n            number = int(number_str) if number_str.isdigit() else 0\n\n            rule = line.split('(')[1].split(')')[0]\n            summary_rules_by_prefix[number].append(rule)\n    return summary_rules_by_prefix\n\ndef calculate_rule_percentage(clingo_output, positive_predictions):\n    summary_rules_by_prefix = defaultdict(list)\n    rule_percentages = {}\n    for line in clingo_output:\n        if line.startswith('rule_predict_pos'):\n            number_str = line.split('(')[0].split('rule_predict_pos')[1]\n            if number_str == '':\n                number_str = '0'\n            number = int(number_str)\n            warning_number = line.split('(')[1].split(')')[0]\n            summary_rules_by_prefix[number].append(warning_number)\n    for rule_number, matched_warnings in summary_rules_by_prefix.items():\n        rule_percentages[rule_number] = len(set(matched_warnings) & set(positive_predictions)) / len(set(matched_warnings))\n    return rule_percentages\n\ndef number_of_rules_over_percentage(percentages, percentage_threshold=0.8):\n    return sum(1 for p in percentages.values() if p >= percentage_threshold)\n\ndef get_number_of_positive_predictions(clingo_output):\n    positive_predictions = set()\n    for line in clingo_output:\n        if line.startswith('rule_predict_pos'):\n            warning_number = line.split('(')[1].split(')')[0]\n            positive_predictions.add(warning_number)\n    return len(positive_predictions)\n\n\ndef get_positive_predictions(clingo_output, rule_numbers):\n    # rule_predict_pos<number>(<warning>)\n    positive_predictions = []\n    for line in clingo_output:\n        if line.startswith('rule_predict_pos'):\n            number_str = line.split('(')[0].split('rule_predict_pos')[1]\n            number = int(number_str) if number_str.isdigit() else 0\n            if number not in rule_numbers:\n                continue\n            # extract warning from parenthesis\n            warning = line.split('(')[1].split(')')[0]\n            if warning not in ground_truth:\n                continue\n            positive_predictions.append(warning)\n    return positive_predictions\n\ndef get_positive_predictions_of_rule(clingo_output, rule_number):\n    # rule_predict_pos<number>(<warning>)\n    positive_predictions = []\n    for line in clingo_output:\n        matches_rule = line.startswith(f'rule_predict_pos{rule_number}') if rule_number != 0 else line.startswith('rule_predict_pos')\n        if matches_rule:\n            # extract warning from parenthesis\n            warning = line.split('(')[1].split(')')[0]\n            if warning not in ground_truth:\n                continue\n            positive_predictions.append(warning)\n    return positive_predictions\n\n# Simulation driver code\n\n# Parameters for the simulation\nwarning_type = sys.argv[1]\nground_truth_file = sys.argv[2]\n\n# Read ground_truth data\nground_truth = read_ground_truth(ground_truth_file)\n\ngraph_id_to_warning = {}\nwith open('meteor_app/private/original_graphs/' + warning_type + '_graph_id_mapping.txt', 'r') as file:\n    for line in file:\n        graph_id = line.split(',')[1]\n        example_id = line.split(',')[2].split(' - ')[0]\n        graph_id_to_warning[graph_id] = example_id\n\n# Load code data from JSON\nwith open('meteor_app/private/original_graphs/' + warning_type + '_elementpositions.json', 'r') as file:\n    code_content_data = json.load(file)\n\ncode_data = {}\n\nfor graph_id_str, another_json_str in code_content_data.items():\n    warning_data = json.loads(another_json_str)\n\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n    try:\n        warning_id = int(graph_id_to_warning[graph_id_str])\n    except:\n        continue\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1]\n    #'Heuristic 2 Only': [2],\n    #'Heuristic 3 Only': [3],\n    #'All Heuristics': [4]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Save the results to a csv file\nwith open(warning_type + '_simulation_results.csv', 'w') as f:\n    f.write('Scenario,Probability,Iteration,Accuracy,Rule Percentage,Conciseness\\n')\n    for scenario_name, p_results in accuracy_results.items():\n        for p_value, accuracy_list in p_results.items():\n            rule_percentage_list = rule_percentage_results[scenario_name][p_value]\n            conciseness_list = conciseness_results[scenario_name][p_value]\n            for i in range(len(accuracy_list)):\n                if i < len(rule_percentage_list) and i < len(conciseness_list):\n                    f.write(f'{scenario_name},{p_value},{i},{accuracy_list[i]},{rule_percentage_list[i]},{conciseness_list[i]}\\n')\n                else:\n                    f.write(f'{scenario_name},{p_value},{i},{accuracy_list[i]},0,0\\n')"],time:'2024-09-06T20:57:18.111Z'},{src:'onDidChangeActiveTerminal',msg:'Current terminal: [%s]; Previous terminal: [%s]',prm:['node','zsh'],time:'2024-09-06T21:00:06.026Z'}]
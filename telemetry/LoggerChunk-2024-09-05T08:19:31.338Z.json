[{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['314','51','315','51','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py','_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle'],time:'2024-09-05T08:19:31.365Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['314','51','315','50','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py','_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handl'],time:'2024-09-05T08:19:31.392Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['0','0','468','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"# the idea is to try different combinations \n# IMPORTANT: prequisites for running this script : run `meteor` in the `meteor_app` directory \n# Run one of \n# e.g., WARNING_TYPE=apache_lucene-solr__NULL_ WARNING_JSON_NAME=spotbugs_warnings_apache_lucene-solr__NULL_ meteor  \n# e.g., WARNING_JSON_NAME=infer_warnings_alibaba_nacos_NULL_DEREFERENCE meteor \n# e.g,, WARNING_TYPE=RESOURCE_LEAK__presto WARNING_JSON_NAME=infer_warnings_prestodb_presto_RESOURCE_LEAK meteor \n# and generate the background.lp file \n\nimport random\nimport subprocess\nimport sys\nimport os\nfrom collections import defaultdict\nimport json\nimport time\nimport matplotlib.pyplot as plt\n\ndef read_ground_truth(ground_truth_file):\n    ground_truth = {}\n    with open(ground_truth_file) as f:\n        for line in f:\n            line = line.strip().split()\n            ground_truth[line[0]] = line[1]\n    return ground_truth\n\ndef write_labels_to_clingo_input(poss, negs):\n    with open('lp/simulation_labels.lp', 'w+') as f:\n        for p in poss:\n            f.write(f'pos({p}).\\n')\n        for n in negs:\n            f.write(f'neg({n}).\\n')\n\ndef read_containment(containment_file='lp/background.lp'):\n    containment = {}\n    last_checked_id = -1  # Initialize last_checked_id to keep track of the last ID we processed\n\n    with open(containment_file) as f:\n        for line in f:\n            line = line.strip().split()\n\n            # If line contains 'containment', extract the warning ID\n            if 'containment' in line[0]:\n                id = int(line[0].split('containment(')[1].split(',')[0])\n\n                # Only process the first occurrence of each ID\n                if id != last_checked_id:\n                    loc = line[1].split(\")\")[0]\n                    loc = loc.replace('__', '.').replace('_', '.')\n                    last_checked_id = id  # Update last_checked_id to prevent re-checking this ID\n                    containment[id] = loc  # Store the location for this ID\n    return containment\n\ndef heuristic_shorter_code_first(warnings):\n    \"\"\"Sort the warnings by lines of code (ascending).\"\"\"\n    return sorted(warnings, key=lambda x: x[1])\n\ndef heuristic_shared_function_calls(warnings):\n    \"\"\"Sort the warnings by the number of shared function calls (descending).\"\"\"\n    def shared_function_calls(warning, other_warnings):\n        return sum(len(set(warning[2]).intersection(set(other[2]))) for other in other_warnings)\n    #print(sorted(warnings, key=lambda x: shared_function_calls(x, warnings), reverse=True))\n    return sorted(warnings, key=lambda x: shared_function_calls(x, warnings), reverse=True)\n\ndef heuristic_neighbor_classes(warnings, containment):\n    \"\"\"Sort the warnings by neighbor classes (contained in the same package or directory).\"\"\"\n    sorted_warnings = []\n    processed_locs = set()\n    \n    for i in range(len(warnings)):\n        loc = containment.get(int(warnings[i][0]))\n        if loc not in processed_locs:\n            # Find all warnings with the same location\n            same_loc_warnings = [warnings[j] for j in range(i, len(warnings)) if containment.get(int(warnings[j][0])) == loc]\n            sorted_warnings.extend(same_loc_warnings)\n            processed_locs.add(loc)\n    \n    return sorted_warnings\n\ndef initialize_warnings_state(ground_truth):\n    \"\"\"Initialize all warnings with the state 'uninspected'.\"\"\"\n    return {k: 'uninspected' for k in ground_truth.keys()}\n\ndef calculate_accuracy(warnings_state, ground_truth):\n    \"\"\"Calculate the accuracy by comparing the current state of warnings with the ground truth.\"\"\"\n    correct_labels = sum(1 for k, v in warnings_state.items() if v == ground_truth[k])\n    return correct_labels / len(ground_truth) * 100\n\ndef sample_labels_randomized_then_sorted(ground_truth, num_pos, num_neg, code_data, warnings_state, apply_heuristics=None):\n    \"\"\"\n    Apply specified heuristics to the warnings before sampling:\n    - apply_heuristics: A list containing the numbers [1, 2, 3] corresponding to the heuristics to apply.\n        1: Review the shorter code first.\n        2: Look for similar code (shared API calls).\n        3: Look for neighbor classes (contained in the same package or directory).\n    - After applying heuristics, select either one positive or one negative warning based on a coin toss.\n    \"\"\"\n    if apply_heuristics is None:\n        apply_heuristics = [1, 2, 3]  # Default to applying all heuristics\n    \n    # Get all uninspected warnings with their lines of code and function calls\n    warnings = [\n        (k, code_data.get(int(k), {}).get('linesOfCode', 0), code_data.get(int(k), {}).get('functionCalls', []))\n        for k, v in ground_truth.items() if warnings_state[k] == 'uninspected'\n    ]\n    \n    if not warnings:  # If all warnings have been inspected, return empty lists\n        return [], []\n    \n    # Apply heuristics in the specified order\n    containment = read_containment()\n    for heuristic in apply_heuristics:\n        if heuristic == 1:\n            warnings = heuristic_shorter_code_first(warnings)\n        elif heuristic == 2:\n            warnings = heuristic_shared_function_calls(warnings)\n        elif heuristic == 3:\n            warnings = heuristic_neighbor_classes(warnings, containment)\n\n    # Sort by code length to identify shortest and longest warnings\n    sorted_warnings_by_length = sorted(warnings, key=lambda x: x[1])\n    \n    # Coin toss to decide whether to pick a positive or negative warning\n    coin_toss = random.choice(['positive', 'negative'])\n    \n    if coin_toss == 'positive':\n        # Select the shortest warning for positive\n        selected_positive_warnings = [sorted_warnings_by_length[0][0]]\n        selected_negative_warnings = []\n        warnings_state[selected_positive_warnings[0]] = 'positive'\n        #print('Coin toss result: Positive selected')\n    else:\n        # Select the longest warning for negative\n        selected_positive_warnings = []\n        selected_negative_warnings = [sorted_warnings_by_length[-1][0]]\n        warnings_state[selected_negative_warnings[0]] = 'negative'\n        #print('Coin toss result: Negative selected')\n    \n    print('Selected positive warning:', selected_positive_warnings)\n    print('Selected negative warning:', selected_negative_warnings)\n    \n    return selected_positive_warnings, selected_negative_warnings\n\n\ndef run_clingo():\n    files = [\n        f'lp/simulation_labels.lp',  \n        'lp/background.lp',              \n        'lp/frozen_rules.lp',            \n        'lp/rules2.lp'                   \n    ]\n    \n    command = ['clingo'] + files + ['--outf=2'] + ['--time-limit=30'] \n    print(' '.join(command))\n    result = subprocess.run(command, capture_output=True, text=True)\n\n    return result.stdout\n\ndef parse_clingo_output(data):\n    # Load the data using json.loads if 'data' is a string,\n    # otherwise assume it's already a dictionary\n    if isinstance(data, str):\n        data = json.loads(data)\n    \n    # Navigate through the JSON structure\n    # Assuming 'Call' is always present and has at least one element\n    calls = data.get(\"Call\", [])\n    if calls:\n        # Assuming 'Witnesses' is always present in the last element of 'Call' and has at least one element\n        last_call = calls[-1]\n        witnesses = last_call.get(\"Witnesses\", [])\n        if witnesses:\n            # Get the last 'Witnesses' entry\n            last_witness = witnesses[-1]\n            # Return the 'Value' list from the last 'Witnesses' entry\n            return last_witness.get(\"Value\", [])\n    return []\n\ndef extract_summary_rules(clingo_output):\n    # a summary rule is one prefixed by rule_contains(number)\n    summary_rules_by_prefix = defaultdict(list)\n    for line in clingo_output:\n        if line.startswith('rule_contains'):\n\n            number_str = line.split('(')[0].split('rule_contains')[1]\n            number = int(number_str) if number_str.isdigit() else 0\n\n            rule = line.split('(')[1].split(')')[0]\n            summary_rules_by_prefix[number].append(rule)\n    return summary_rules_by_prefix\n\ndef calculate_rule_percentage(clingo_output, positive_predictions):\n    summary_rules_by_prefix = defaultdict(list)\n    rule_percentages = {}\n    for line in clingo_output:\n        if line.startswith('rule_predict_pos'):\n            number_str = line.split('(')[0].split('rule_predict_pos')[1]\n            if number_str == '':\n                number_str = '0'\n            number = int(number_str)\n            warning_number = line.split('(')[1].split(')')[0]\n            summary_rules_by_prefix[number].append(warning_number)\n    for rule_number, matched_warnings in summary_rules_by_prefix.items():\n        rule_percentages[rule_number] = len(set(matched_warnings) & set(positive_predictions)) / len(set(matched_warnings))\n    return rule_percentages\n\ndef number_of_rules_over_percentage(percentages, percentage_threshold=0.8):\n    return sum(1 for p in percentages.values() if p >= percentage_threshold)\n\ndef get_number_of_positive_predictions(clingo_output):\n    positive_predictions = set()\n    for line in clingo_output:\n        if line.startswith('rule_predict_pos'):\n            warning_number = line.split('(')[1].split(')')[0]\n            positive_predictions.add(warning_number)\n    return len(positive_predictions)\n\n\ndef get_positive_predictions(clingo_output, rule_numbers):\n    # rule_predict_pos<number>(<warning>)\n    positive_predictions = []\n    for line in clingo_output:\n        if line.startswith('rule_predict_pos'):\n            number_str = line.split('(')[0].split('rule_predict_pos')[1]\n            number = int(number_str) if number_str.isdigit() else 0\n            if number not in rule_numbers:\n                continue\n            # extract warning from parenthesis\n            warning = line.split('(')[1].split(')')[0]\n            positive_predictions.append(warning)\n    return positive_predictions\n\ndef get_positive_predictions_of_rule(clingo_output, rule_number):\n    # rule_predict_pos<number>(<warning>)\n    positive_predictions = []\n    for line in clingo_output:\n        matches_rule = line.startswith(f'rule_predict_pos{rule_number}') if rule_number != 0 else line.startswith('rule_predict_pos')\n        if matches_rule:\n            # extract warning from parenthesis\n            warning = line.split('(')[1].split(')')[0]\n            positive_predictions.append(warning)\n    return positive_predictions\n\n# Simulation driver code\n\n# Parameters for the simulation\nwarning_type = sys.argv[1]\nground_truth_file = sys.argv[2]\n\n# Read ground_truth data\nground_truth = read_ground_truth(ground_truth_file)\n\ngraph_id_to_warning = {}\nwith open('meteor_app/private/original_graphs/' + warning_type + '_graph_id_mapping.txt', 'r') as file:\n    for line in file:\n        graph_id = line.split(',')[1]\n        example_id = line.split(',')[2].split(' - ')[0]\n        graph_id_to_warning[graph_id] = example_id\n\n# Load code data from JSON\nwith open('meteor_app/private/original_graphs/' + warning_type + '_elementpositions.json', 'r') as file:\n    code_content_data = json.load(file)\n\ncode_data = {}\n\nfor graph_id_str, another_json_str in code_content_data.items():\n    warning_data = json.loads(another_json_str)\n\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n"],time:'2024-09-05T08:19:31.744Z'},{src:'onDidChangeTextDocument',msg:'%s:%s to %s:%s in [%s] replaced with: %s`',prm:['377','0','468','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py.git',"            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:37.898Z'},{src:'onDidChangeTextDocument',msg:'%s:%s to %s:%s in [%s] replaced with: %s`',prm:['377','0','468','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:39.172Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['0','0','421','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"# the idea is to try different combinations \n# IMPORTANT: prequisites for running this script : run `meteor` in the `meteor_app` directory \n# Run one of \n# e.g., WARNING_TYPE=apache_lucene-solr__NULL_ WARNING_JSON_NAME=spotbugs_warnings_apache_lucene-solr__NULL_ meteor  \n# e.g., WARNING_JSON_NAME=infer_warnings_alibaba_nacos_NULL_DEREFERENCE meteor \n# e.g,, WARNING_TYPE=RESOURCE_LEAK__presto WARNING_JSON_NAME=infer_warnings_prestodb_presto_RESOURCE_LEAK meteor \n# and generate the background.lp file \n\nimport random\nimport subprocess\nimport sys\nimport os\nfrom collections import defaultdict\nimport json\nimport time\nimport matplotlib.pyplot as plt\n\ndef read_ground_truth(ground_truth_file):\n    ground_truth = {}\n    with open(ground_truth_file) as f:\n        for line in f:\n            line = line.strip().split()\n            ground_truth[line[0]] = line[1]\n    return ground_truth\n\ndef write_labels_to_clingo_input(poss, negs):\n    with open('lp/simulation_labels.lp', 'w+') as f:\n        for p in poss:\n            f.write(f'pos({p}).\\n')\n        for n in negs:\n            f.write(f'neg({n}).\\n')\n\ndef read_containment(containment_file='lp/background.lp'):\n    containment = {}\n    last_checked_id = -1  # Initialize last_checked_id to keep track of the last ID we processed\n\n    with open(containment_file) as f:\n        for line in f:\n            line = line.strip().split()\n\n            # If line contains 'containment', extract the warning ID\n            if 'containment' in line[0]:\n                id = int(line[0].split('containment(')[1].split(',')[0])\n\n                # Only process the first occurrence of each ID\n                if id != last_checked_id:\n                    loc = line[1].split(\")\")[0]\n                    loc = loc.replace('__', '.').replace('_', '.')\n                    last_checked_id = id  # Update last_checked_id to prevent re-checking this ID\n                    containment[id] = loc  # Store the location for this ID\n    return containment\n\ndef heuristic_shorter_code_first(warnings):\n    \"\"\"Sort the warnings by lines of code (ascending).\"\"\"\n    return sorted(warnings, key=lambda x: x[1])\n\ndef heuristic_shared_function_calls(warnings):\n    \"\"\"Sort the warnings by the number of shared function calls (descending).\"\"\"\n    def shared_function_calls(warning, other_warnings):\n        return sum(len(set(warning[2]).intersection(set(other[2]))) for other in other_warnings)\n    #print(sorted(warnings, key=lambda x: shared_function_calls(x, warnings), reverse=True))\n    return sorted(warnings, key=lambda x: shared_function_calls(x, warnings), reverse=True)\n\ndef heuristic_neighbor_classes(warnings, containment):\n    \"\"\"Sort the warnings by neighbor classes (contained in the same package or directory).\"\"\"\n    sorted_warnings = []\n    processed_locs = set()\n    \n    for i in range(len(warnings)):\n        loc = containment.get(int(warnings[i][0]))\n        if loc not in processed_locs:\n            # Find all warnings with the same location\n            same_loc_warnings = [warnings[j] for j in range(i, len(warnings)) if containment.get(int(warnings[j][0])) == loc]\n            sorted_warnings.extend(same_loc_warnings)\n            processed_locs.add(loc)\n    \n    return sorted_warnings\n\ndef initialize_warnings_state(ground_truth):\n    \"\"\"Initialize all warnings with the state 'uninspected'.\"\"\"\n    return {k: 'uninspected' for k in ground_truth.keys()}\n\ndef calculate_accuracy(warnings_state, ground_truth):\n    \"\"\"Calculate the accuracy by comparing the current state of warnings with the ground truth.\"\"\"\n    correct_labels = sum(1 for k, v in warnings_state.items() if v == ground_truth[k])\n    return correct_labels / len(ground_truth) * 100\n\ndef sample_labels_randomized_then_sorted(ground_truth, num_pos, num_neg, code_data, warnings_state, apply_heuristics=None):\n    \"\"\"\n    Apply specified heuristics to the warnings before sampling:\n    - apply_heuristics: A list containing the numbers [1, 2, 3] corresponding to the heuristics to apply.\n        1: Review the shorter code first.\n        2: Look for similar code (shared API calls).\n        3: Look for neighbor classes (contained in the same package or directory).\n    - After applying heuristics, select either one positive or one negative warning based on a coin toss.\n    \"\"\"\n    if apply_heuristics is None:\n        apply_heuristics = [1, 2, 3]  # Default to applying all heuristics\n    \n    # Get all uninspected warnings with their lines of code and function calls\n    warnings = [\n        (k, code_data.get(int(k), {}).get('linesOfCode', 0), code_data.get(int(k), {}).get('functionCalls', []))\n        for k, v in ground_truth.items() if warnings_state[k] == 'uninspected'\n    ]\n    \n    if not warnings:  # If all warnings have been inspected, return empty lists\n        return [], []\n    \n    # Apply heuristics in the specified order\n    containment = read_containment()\n    for heuristic in apply_heuristics:\n        if heuristic == 1:\n            warnings = heuristic_shorter_code_first(warnings)\n        elif heuristic == 2:\n            warnings = heuristic_shared_function_calls(warnings)\n        elif heuristic == 3:\n            warnings = heuristic_neighbor_classes(warnings, containment)\n\n    # Sort by code length to identify shortest and longest warnings\n    sorted_warnings_by_length = sorted(warnings, key=lambda x: x[1])\n    \n    # Coin toss to decide whether to pick a positive or negative warning\n    coin_toss = random.choice(['positive', 'negative'])\n    \n    if coin_toss == 'positive':\n        # Select the shortest warning for positive\n        selected_positive_warnings = [sorted_warnings_by_length[0][0]]\n        selected_negative_warnings = []\n        warnings_state[selected_positive_warnings[0]] = 'positive'\n        #print('Coin toss result: Positive selected')\n    else:\n        # Select the longest warning for negative\n        selected_positive_warnings = []\n        selected_negative_warnings = [sorted_warnings_by_length[-1][0]]\n        warnings_state[selected_negative_warnings[0]] = 'negative'\n        #print('Coin toss result: Negative selected')\n    \n    print('Selected positive warning:', selected_positive_warnings)\n    print('Selected negative warning:', selected_negative_warnings)\n    \n    return selected_positive_warnings, selected_negative_warnings\n\n\ndef run_clingo():\n    files = [\n        f'lp/simulation_labels.lp',  \n        'lp/background.lp',              \n        'lp/frozen_rules.lp',            \n        'lp/rules2.lp'                   \n    ]\n    \n    command = ['clingo'] + files + ['--outf=2'] + ['--time-limit=30'] \n    print(' '.join(command))\n    result = subprocess.run(command, capture_output=True, text=True)\n\n    return result.stdout\n\ndef parse_clingo_output(data):\n    # Load the data using json.loads if 'data' is a string,\n    # otherwise assume it's already a dictionary\n    if isinstance(data, str):\n        data = json.loads(data)\n    \n    # Navigate through the JSON structure\n    # Assuming 'Call' is always present and has at least one element\n    calls = data.get(\"Call\", [])\n    if calls:\n        # Assuming 'Witnesses' is always present in the last element of 'Call' and has at least one element\n        last_call = calls[-1]\n        witnesses = last_call.get(\"Witnesses\", [])\n        if witnesses:\n            # Get the last 'Witnesses' entry\n            last_witness = witnesses[-1]\n            # Return the 'Value' list from the last 'Witnesses' entry\n            return last_witness.get(\"Value\", [])\n    return []\n\ndef extract_summary_rules(clingo_output):\n    # a summary rule is one prefixed by rule_contains(number)\n    summary_rules_by_prefix = defaultdict(list)\n    for line in clingo_output:\n        if line.startswith('rule_contains'):\n\n            number_str = line.split('(')[0].split('rule_contains')[1]\n            number = int(number_str) if number_str.isdigit() else 0\n\n            rule = line.split('(')[1].split(')')[0]\n            summary_rules_by_prefix[number].append(rule)\n    return summary_rules_by_prefix\n\ndef calculate_rule_percentage(clingo_output, positive_predictions):\n    summary_rules_by_prefix = defaultdict(list)\n    rule_percentages = {}\n    for line in clingo_output:\n        if line.startswith('rule_predict_pos'):\n            number_str = line.split('(')[0].split('rule_predict_pos')[1]\n            if number_str == '':\n                number_str = '0'\n            number = int(number_str)\n            warning_number = line.split('(')[1].split(')')[0]\n            summary_rules_by_prefix[number].append(warning_number)\n    for rule_number, matched_warnings in summary_rules_by_prefix.items():\n        rule_percentages[rule_number] = len(set(matched_warnings) & set(positive_predictions)) / len(set(matched_warnings))\n    return rule_percentages\n\ndef number_of_rules_over_percentage(percentages, percentage_threshold=0.8):\n    return sum(1 for p in percentages.values() if p >= percentage_threshold)\n\ndef get_number_of_positive_predictions(clingo_output):\n    positive_predictions = set()\n    for line in clingo_output:\n        if line.startswith('rule_predict_pos'):\n            warning_number = line.split('(')[1].split(')')[0]\n            positive_predictions.add(warning_number)\n    return len(positive_predictions)\n\n\ndef get_positive_predictions(clingo_output, rule_numbers):\n    # rule_predict_pos<number>(<warning>)\n    positive_predictions = []\n    for line in clingo_output:\n        if line.startswith('rule_predict_pos'):\n            number_str = line.split('(')[0].split('rule_predict_pos')[1]\n            number = int(number_str) if number_str.isdigit() else 0\n            if number not in rule_numbers:\n                continue\n            # extract warning from parenthesis\n            warning = line.split('(')[1].split(')')[0]\n            positive_predictions.append(warning)\n    return positive_predictions\n\ndef get_positive_predictions_of_rule(clingo_output, rule_number):\n    # rule_predict_pos<number>(<warning>)\n    positive_predictions = []\n    for line in clingo_output:\n        matches_rule = line.startswith(f'rule_predict_pos{rule_number}') if rule_number != 0 else line.startswith('rule_predict_pos')\n        if matches_rule:\n            # extract warning from parenthesis\n            warning = line.split('(')[1].split(')')[0]\n            positive_predictions.append(warning)\n    return positive_predictions\n\n# Simulation driver code\n\n# Parameters for the simulation\nwarning_type = sys.argv[1]\nground_truth_file = sys.argv[2]\n\n# Read ground_truth data\nground_truth = read_ground_truth(ground_truth_file)\n\ngraph_id_to_warning = {}\nwith open('meteor_app/private/original_graphs/' + warning_type + '_graph_id_mapping.txt', 'r') as file:\n    for line in file:\n        graph_id = line.split(',')[1]\n        example_id = line.split(',')[2].split(' - ')[0]\n        graph_id_to_warning[graph_id] = example_id\n\n# Load code data from JSON\nwith open('meteor_app/private/original_graphs/' + warning_type + '_elementpositions.json', 'r') as file:\n    code_content_data = json.load(file)\n\ncode_data = {}\n\nfor graph_id_str, another_json_str in code_content_data.items():\n    warning_data = json.loads(another_json_str)\n\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:39.174Z'},{src:'onDidChangeTextDocument',msg:'%s:%s to %s:%s in [%s] replaced with: %s`',prm:['270','0','272','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py','    try:\n        warning_id = int(graph_id_to_warning[graph_id_str])\n    except:\n        continue\n'],time:'2024-09-05T08:19:43.714Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['0','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"# the idea is to try different combinations \n# IMPORTANT: prequisites for running this script : run `meteor` in the `meteor_app` directory \n# Run one of \n# e.g., WARNING_TYPE=apache_lucene-solr__NULL_ WARNING_JSON_NAME=spotbugs_warnings_apache_lucene-solr__NULL_ meteor  \n# e.g., WARNING_JSON_NAME=infer_warnings_alibaba_nacos_NULL_DEREFERENCE meteor \n# e.g,, WARNING_TYPE=RESOURCE_LEAK__presto WARNING_JSON_NAME=infer_warnings_prestodb_presto_RESOURCE_LEAK meteor \n# and generate the background.lp file \n\nimport random\nimport subprocess\nimport sys\nimport os\nfrom collections import defaultdict\nimport json\nimport time\nimport matplotlib.pyplot as plt\n\ndef read_ground_truth(ground_truth_file):\n    ground_truth = {}\n    with open(ground_truth_file) as f:\n        for line in f:\n            line = line.strip().split()\n            ground_truth[line[0]] = line[1]\n    return ground_truth\n\ndef write_labels_to_clingo_input(poss, negs):\n    with open('lp/simulation_labels.lp', 'w+') as f:\n        for p in poss:\n            f.write(f'pos({p}).\\n')\n        for n in negs:\n            f.write(f'neg({n}).\\n')\n\ndef read_containment(containment_file='lp/background.lp'):\n    containment = {}\n    last_checked_id = -1  # Initialize last_checked_id to keep track of the last ID we processed\n\n    with open(containment_file) as f:\n        for line in f:\n            line = line.strip().split()\n\n            # If line contains 'containment', extract the warning ID\n            if 'containment' in line[0]:\n                id = int(line[0].split('containment(')[1].split(',')[0])\n\n                # Only process the first occurrence of each ID\n                if id != last_checked_id:\n                    loc = line[1].split(\")\")[0]\n                    loc = loc.replace('__', '.').replace('_', '.')\n                    last_checked_id = id  # Update last_checked_id to prevent re-checking this ID\n                    containment[id] = loc  # Store the location for this ID\n    return containment\n\ndef heuristic_shorter_code_first(warnings):\n    \"\"\"Sort the warnings by lines of code (ascending).\"\"\"\n    return sorted(warnings, key=lambda x: x[1])\n\ndef heuristic_shared_function_calls(warnings):\n    \"\"\"Sort the warnings by the number of shared function calls (descending).\"\"\"\n    def shared_function_calls(warning, other_warnings):\n        return sum(len(set(warning[2]).intersection(set(other[2]))) for other in other_warnings)\n    #print(sorted(warnings, key=lambda x: shared_function_calls(x, warnings), reverse=True))\n    return sorted(warnings, key=lambda x: shared_function_calls(x, warnings), reverse=True)\n\ndef heuristic_neighbor_classes(warnings, containment):\n    \"\"\"Sort the warnings by neighbor classes (contained in the same package or directory).\"\"\"\n    sorted_warnings = []\n    processed_locs = set()\n    \n    for i in range(len(warnings)):\n        loc = containment.get(int(warnings[i][0]))\n        if loc not in processed_locs:\n            # Find all warnings with the same location\n            same_loc_warnings = [warnings[j] for j in range(i, len(warnings)) if containment.get(int(warnings[j][0])) == loc]\n            sorted_warnings.extend(same_loc_warnings)\n            processed_locs.add(loc)\n    \n    return sorted_warnings\n\ndef initialize_warnings_state(ground_truth):\n    \"\"\"Initialize all warnings with the state 'uninspected'.\"\"\"\n    return {k: 'uninspected' for k in ground_truth.keys()}\n\ndef calculate_accuracy(warnings_state, ground_truth):\n    \"\"\"Calculate the accuracy by comparing the current state of warnings with the ground truth.\"\"\"\n    correct_labels = sum(1 for k, v in warnings_state.items() if v == ground_truth[k])\n    return correct_labels / len(ground_truth) * 100\n\ndef sample_labels_randomized_then_sorted(ground_truth, num_pos, num_neg, code_data, warnings_state, apply_heuristics=None):\n    \"\"\"\n    Apply specified heuristics to the warnings before sampling:\n    - apply_heuristics: A list containing the numbers [1, 2, 3] corresponding to the heuristics to apply.\n        1: Review the shorter code first.\n        2: Look for similar code (shared API calls).\n        3: Look for neighbor classes (contained in the same package or directory).\n    - After applying heuristics, select either one positive or one negative warning based on a coin toss.\n    \"\"\"\n    if apply_heuristics is None:\n        apply_heuristics = [1, 2, 3]  # Default to applying all heuristics\n    \n    # Get all uninspected warnings with their lines of code and function calls\n    warnings = [\n        (k, code_data.get(int(k), {}).get('linesOfCode', 0), code_data.get(int(k), {}).get('functionCalls', []))\n        for k, v in ground_truth.items() if warnings_state[k] == 'uninspected'\n    ]\n    \n    if not warnings:  # If all warnings have been inspected, return empty lists\n        return [], []\n    \n    # Apply heuristics in the specified order\n    containment = read_containment()\n    for heuristic in apply_heuristics:\n        if heuristic == 1:\n            warnings = heuristic_shorter_code_first(warnings)\n        elif heuristic == 2:\n            warnings = heuristic_shared_function_calls(warnings)\n        elif heuristic == 3:\n            warnings = heuristic_neighbor_classes(warnings, containment)\n\n    # Sort by code length to identify shortest and longest warnings\n    sorted_warnings_by_length = sorted(warnings, key=lambda x: x[1])\n    \n    # Coin toss to decide whether to pick a positive or negative warning\n    coin_toss = random.choice(['positive', 'negative'])\n    \n    if coin_toss == 'positive':\n        # Select the shortest warning for positive\n        selected_positive_warnings = [sorted_warnings_by_length[0][0]]\n        selected_negative_warnings = []\n        warnings_state[selected_positive_warnings[0]] = 'positive'\n        #print('Coin toss result: Positive selected')\n    else:\n        # Select the longest warning for negative\n        selected_positive_warnings = []\n        selected_negative_warnings = [sorted_warnings_by_length[-1][0]]\n        warnings_state[selected_negative_warnings[0]] = 'negative'\n        #print('Coin toss result: Negative selected')\n    \n    print('Selected positive warning:', selected_positive_warnings)\n    print('Selected negative warning:', selected_negative_warnings)\n    \n    return selected_positive_warnings, selected_negative_warnings\n\n\ndef run_clingo():\n    files = [\n        f'lp/simulation_labels.lp',  \n        'lp/background.lp',              \n        'lp/frozen_rules.lp',            \n        'lp/rules2.lp'                   \n    ]\n    \n    command = ['clingo'] + files + ['--outf=2'] + ['--time-limit=30'] \n    print(' '.join(command))\n    result = subprocess.run(command, capture_output=True, text=True)\n\n    return result.stdout\n\ndef parse_clingo_output(data):\n    # Load the data using json.loads if 'data' is a string,\n    # otherwise assume it's already a dictionary\n    if isinstance(data, str):\n        data = json.loads(data)\n    \n    # Navigate through the JSON structure\n    # Assuming 'Call' is always present and has at least one element\n    calls = data.get(\"Call\", [])\n    if calls:\n        # Assuming 'Witnesses' is always present in the last element of 'Call' and has at least one element\n        last_call = calls[-1]\n        witnesses = last_call.get(\"Witnesses\", [])\n        if witnesses:\n            # Get the last 'Witnesses' entry\n            last_witness = witnesses[-1]\n            # Return the 'Value' list from the last 'Witnesses' entry\n            return last_witness.get(\"Value\", [])\n    return []\n\ndef extract_summary_rules(clingo_output):\n    # a summary rule is one prefixed by rule_contains(number)\n    summary_rules_by_prefix = defaultdict(list)\n    for line in clingo_output:\n        if line.startswith('rule_contains'):\n\n            number_str = line.split('(')[0].split('rule_contains')[1]\n            number = int(number_str) if number_str.isdigit() else 0\n\n            rule = line.split('(')[1].split(')')[0]\n            summary_rules_by_prefix[number].append(rule)\n    return summary_rules_by_prefix\n\ndef calculate_rule_percentage(clingo_output, positive_predictions):\n    summary_rules_by_prefix = defaultdict(list)\n    rule_percentages = {}\n    for line in clingo_output:\n        if line.startswith('rule_predict_pos'):\n            number_str = line.split('(')[0].split('rule_predict_pos')[1]\n            if number_str == '':\n                number_str = '0'\n            number = int(number_str)\n            warning_number = line.split('(')[1].split(')')[0]\n            summary_rules_by_prefix[number].append(warning_number)\n    for rule_number, matched_warnings in summary_rules_by_prefix.items():\n        rule_percentages[rule_number] = len(set(matched_warnings) & set(positive_predictions)) / len(set(matched_warnings))\n    return rule_percentages\n\ndef number_of_rules_over_percentage(percentages, percentage_threshold=0.8):\n    return sum(1 for p in percentages.values() if p >= percentage_threshold)\n\ndef get_number_of_positive_predictions(clingo_output):\n    positive_predictions = set()\n    for line in clingo_output:\n        if line.startswith('rule_predict_pos'):\n            warning_number = line.split('(')[1].split(')')[0]\n            positive_predictions.add(warning_number)\n    return len(positive_predictions)\n\n\ndef get_positive_predictions(clingo_output, rule_numbers):\n    # rule_predict_pos<number>(<warning>)\n    positive_predictions = []\n    for line in clingo_output:\n        if line.startswith('rule_predict_pos'):\n            number_str = line.split('(')[0].split('rule_predict_pos')[1]\n            number = int(number_str) if number_str.isdigit() else 0\n            if number not in rule_numbers:\n                continue\n            # extract warning from parenthesis\n            warning = line.split('(')[1].split(')')[0]\n            positive_predictions.append(warning)\n    return positive_predictions\n\ndef get_positive_predictions_of_rule(clingo_output, rule_number):\n    # rule_predict_pos<number>(<warning>)\n    positive_predictions = []\n    for line in clingo_output:\n        matches_rule = line.startswith(f'rule_predict_pos{rule_number}') if rule_number != 0 else line.startswith('rule_predict_pos')\n        if matches_rule:\n            # extract warning from parenthesis\n            warning = line.split('(')[1].split(')')[0]\n            positive_predictions.append(warning)\n    return positive_predictions\n\n# Simulation driver code\n\n# Parameters for the simulation\nwarning_type = sys.argv[1]\nground_truth_file = sys.argv[2]\n\n# Read ground_truth data\nground_truth = read_ground_truth(ground_truth_file)\n\ngraph_id_to_warning = {}\nwith open('meteor_app/private/original_graphs/' + warning_type + '_graph_id_mapping.txt', 'r') as file:\n    for line in file:\n        graph_id = line.split(',')[1]\n        example_id = line.split(',')[2].split(' - ')[0]\n        graph_id_to_warning[graph_id] = example_id\n\n# Load code data from JSON\nwith open('meteor_app/private/original_graphs/' + warning_type + '_elementpositions.json', 'r') as file:\n    code_content_data = json.load(file)\n\ncode_data = {}\n\nfor graph_id_str, another_json_str in code_content_data.items():\n    warning_data = json.loads(another_json_str)\n\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n    try:\n        warning_id = int(graph_id_to_warning[graph_id_str])\n    except:\n        continue\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:43.716Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['300','0','326','32','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:19:43.716Z'},{src:'onDidChangeTextDocument',msg:'%s:%s to %s:%s in [%s] replaced with: %s`',prm:['270','0','272','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py.git','    try:\n        warning_id = int(graph_id_to_warning[graph_id_str])\n    except:\n        continue\n'],time:'2024-09-05T08:19:44.936Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['309','0','334','111','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:19:45.258Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['317','0','343','21','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:19:45.323Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['325','0','351','82','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:19:45.408Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['331','0','356','22','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:19:45.473Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['339','0','365','91','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:19:45.540Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['348','0','373','39','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:19:45.623Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['356','0','381','75','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:19:45.874Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['364','0','390','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:19:45.890Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['375','0','401','59','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:19:45.907Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['389','0','415','114','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:19:45.923Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['406','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:19:45.939Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['423','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:19:45.956Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['415','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:19:46.370Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['407','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:19:46.424Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['398','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:19:46.473Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['407','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:19:46.807Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['415','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:19:46.824Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['423','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:19:46.840Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['415','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:19:47.156Z'},{src:'onDidChangeActiveTextEditor',msg:'Current editor: [%s]; Previous editor: [%s]',prm:['','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:19:50.951Z'},{src:'onDidChangeActiveTextEditor',msg:'Current editor: [%s]; Previous editor: [%s]',prm:['/Users/burakyetistiren/Desktop/warning_suppression/code/lp/background.lp',''],time:'2024-09-05T08:19:51.005Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['78','0','104','70','/Users/burakyetistiren/Desktop/warning_suppression/code/lp/background.lp'],time:'2024-09-05T08:19:51.016Z'},{src:'onDidChangeActiveTextEditor',msg:'Current editor: [%s]; Previous editor: [%s]',prm:['','/Users/burakyetistiren/Desktop/warning_suppression/code/lp/background.lp'],time:'2024-09-05T08:19:51.854Z'},{src:'onDidChangeActiveTextEditor',msg:'Current editor: [%s]; Previous editor: [%s]',prm:['/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',''],time:'2024-09-05T08:19:51.873Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['0','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"# the idea is to try different combinations \n# IMPORTANT: prequisites for running this script : run `meteor` in the `meteor_app` directory \n# Run one of \n# e.g., WARNING_TYPE=apache_lucene-solr__NULL_ WARNING_JSON_NAME=spotbugs_warnings_apache_lucene-solr__NULL_ meteor  \n# e.g., WARNING_JSON_NAME=infer_warnings_alibaba_nacos_NULL_DEREFERENCE meteor \n# e.g,, WARNING_TYPE=RESOURCE_LEAK__presto WARNING_JSON_NAME=infer_warnings_prestodb_presto_RESOURCE_LEAK meteor \n# and generate the background.lp file \n\nimport random\nimport subprocess\nimport sys\nimport os\nfrom collections import defaultdict\nimport json\nimport time\nimport matplotlib.pyplot as plt\n\ndef read_ground_truth(ground_truth_file):\n    ground_truth = {}\n    with open(ground_truth_file) as f:\n        for line in f:\n            line = line.strip().split()\n            ground_truth[line[0]] = line[1]\n    return ground_truth\n\ndef write_labels_to_clingo_input(poss, negs):\n    with open('lp/simulation_labels.lp', 'w+') as f:\n        for p in poss:\n            f.write(f'pos({p}).\\n')\n        for n in negs:\n            f.write(f'neg({n}).\\n')\n\ndef read_containment(containment_file='lp/background.lp'):\n    containment = {}\n    last_checked_id = -1  # Initialize last_checked_id to keep track of the last ID we processed\n\n    with open(containment_file) as f:\n        for line in f:\n            line = line.strip().split()\n\n            # If line contains 'containment', extract the warning ID\n            if 'containment' in line[0]:\n                id = int(line[0].split('containment(')[1].split(',')[0])\n\n                # Only process the first occurrence of each ID\n                if id != last_checked_id:\n                    loc = line[1].split(\")\")[0]\n                    loc = loc.replace('__', '.').replace('_', '.')\n                    last_checked_id = id  # Update last_checked_id to prevent re-checking this ID\n                    containment[id] = loc  # Store the location for this ID\n    return containment\n\ndef heuristic_shorter_code_first(warnings):\n    \"\"\"Sort the warnings by lines of code (ascending).\"\"\"\n    return sorted(warnings, key=lambda x: x[1])\n\ndef heuristic_shared_function_calls(warnings):\n    \"\"\"Sort the warnings by the number of shared function calls (descending).\"\"\"\n    def shared_function_calls(warning, other_warnings):\n        return sum(len(set(warning[2]).intersection(set(other[2]))) for other in other_warnings)\n    #print(sorted(warnings, key=lambda x: shared_function_calls(x, warnings), reverse=True))\n    return sorted(warnings, key=lambda x: shared_function_calls(x, warnings), reverse=True)\n\ndef heuristic_neighbor_classes(warnings, containment):\n    \"\"\"Sort the warnings by neighbor classes (contained in the same package or directory).\"\"\"\n    sorted_warnings = []\n    processed_locs = set()\n    \n    for i in range(len(warnings)):\n        loc = containment.get(int(warnings[i][0]))\n        if loc not in processed_locs:\n            # Find all warnings with the same location\n            same_loc_warnings = [warnings[j] for j in range(i, len(warnings)) if containment.get(int(warnings[j][0])) == loc]\n            sorted_warnings.extend(same_loc_warnings)\n            processed_locs.add(loc)\n    \n    return sorted_warnings\n\ndef initialize_warnings_state(ground_truth):\n    \"\"\"Initialize all warnings with the state 'uninspected'.\"\"\"\n    return {k: 'uninspected' for k in ground_truth.keys()}\n\ndef calculate_accuracy(warnings_state, ground_truth):\n    \"\"\"Calculate the accuracy by comparing the current state of warnings with the ground truth.\"\"\"\n    correct_labels = sum(1 for k, v in warnings_state.items() if v == ground_truth[k])\n    return correct_labels / len(ground_truth) * 100\n\ndef sample_labels_randomized_then_sorted(ground_truth, num_pos, num_neg, code_data, warnings_state, apply_heuristics=None):\n    \"\"\"\n    Apply specified heuristics to the warnings before sampling:\n    - apply_heuristics: A list containing the numbers [1, 2, 3] corresponding to the heuristics to apply.\n        1: Review the shorter code first.\n        2: Look for similar code (shared API calls).\n        3: Look for neighbor classes (contained in the same package or directory).\n    - After applying heuristics, select either one positive or one negative warning based on a coin toss.\n    \"\"\"\n    if apply_heuristics is None:\n        apply_heuristics = [1, 2, 3]  # Default to applying all heuristics\n    \n    # Get all uninspected warnings with their lines of code and function calls\n    warnings = [\n        (k, code_data.get(int(k), {}).get('linesOfCode', 0), code_data.get(int(k), {}).get('functionCalls', []))\n        for k, v in ground_truth.items() if warnings_state[k] == 'uninspected'\n    ]\n    \n    if not warnings:  # If all warnings have been inspected, return empty lists\n        return [], []\n    \n    # Apply heuristics in the specified order\n    containment = read_containment()\n    for heuristic in apply_heuristics:\n        if heuristic == 1:\n            warnings = heuristic_shorter_code_first(warnings)\n        elif heuristic == 2:\n            warnings = heuristic_shared_function_calls(warnings)\n        elif heuristic == 3:\n            warnings = heuristic_neighbor_classes(warnings, containment)\n\n    # Sort by code length to identify shortest and longest warnings\n    sorted_warnings_by_length = sorted(warnings, key=lambda x: x[1])\n    \n    # Coin toss to decide whether to pick a positive or negative warning\n    coin_toss = random.choice(['positive', 'negative'])\n    \n    if coin_toss == 'positive':\n        # Select the shortest warning for positive\n        selected_positive_warnings = [sorted_warnings_by_length[0][0]]\n        selected_negative_warnings = []\n        warnings_state[selected_positive_warnings[0]] = 'positive'\n        #print('Coin toss result: Positive selected')\n    else:\n        # Select the longest warning for negative\n        selected_positive_warnings = []\n        selected_negative_warnings = [sorted_warnings_by_length[-1][0]]\n        warnings_state[selected_negative_warnings[0]] = 'negative'\n        #print('Coin toss result: Negative selected')\n    \n    print('Selected positive warning:', selected_positive_warnings)\n    print('Selected negative warning:', selected_negative_warnings)\n    \n    return selected_positive_warnings, selected_negative_warnings\n\n\ndef run_clingo():\n    files = [\n        f'lp/simulation_labels.lp',  \n        'lp/background.lp',              \n        'lp/frozen_rules.lp',            \n        'lp/rules2.lp'                   \n    ]\n    \n    command = ['clingo'] + files + ['--outf=2'] + ['--time-limit=30'] \n    print(' '.join(command))\n    result = subprocess.run(command, capture_output=True, text=True)\n\n    return result.stdout\n\ndef parse_clingo_output(data):\n    # Load the data using json.loads if 'data' is a string,\n    # otherwise assume it's already a dictionary\n    if isinstance(data, str):\n        data = json.loads(data)\n    \n    # Navigate through the JSON structure\n    # Assuming 'Call' is always present and has at least one element\n    calls = data.get(\"Call\", [])\n    if calls:\n        # Assuming 'Witnesses' is always present in the last element of 'Call' and has at least one element\n        last_call = calls[-1]\n        witnesses = last_call.get(\"Witnesses\", [])\n        if witnesses:\n            # Get the last 'Witnesses' entry\n            last_witness = witnesses[-1]\n            # Return the 'Value' list from the last 'Witnesses' entry\n            return last_witness.get(\"Value\", [])\n    return []\n\ndef extract_summary_rules(clingo_output):\n    # a summary rule is one prefixed by rule_contains(number)\n    summary_rules_by_prefix = defaultdict(list)\n    for line in clingo_output:\n        if line.startswith('rule_contains'):\n\n            number_str = line.split('(')[0].split('rule_contains')[1]\n            number = int(number_str) if number_str.isdigit() else 0\n\n            rule = line.split('(')[1].split(')')[0]\n            summary_rules_by_prefix[number].append(rule)\n    return summary_rules_by_prefix\n\ndef calculate_rule_percentage(clingo_output, positive_predictions):\n    summary_rules_by_prefix = defaultdict(list)\n    rule_percentages = {}\n    for line in clingo_output:\n        if line.startswith('rule_predict_pos'):\n            number_str = line.split('(')[0].split('rule_predict_pos')[1]\n            if number_str == '':\n                number_str = '0'\n            number = int(number_str)\n            warning_number = line.split('(')[1].split(')')[0]\n            summary_rules_by_prefix[number].append(warning_number)\n    for rule_number, matched_warnings in summary_rules_by_prefix.items():\n        rule_percentages[rule_number] = len(set(matched_warnings) & set(positive_predictions)) / len(set(matched_warnings))\n    return rule_percentages\n\ndef number_of_rules_over_percentage(percentages, percentage_threshold=0.8):\n    return sum(1 for p in percentages.values() if p >= percentage_threshold)\n\ndef get_number_of_positive_predictions(clingo_output):\n    positive_predictions = set()\n    for line in clingo_output:\n        if line.startswith('rule_predict_pos'):\n            warning_number = line.split('(')[1].split(')')[0]\n            positive_predictions.add(warning_number)\n    return len(positive_predictions)\n\n\ndef get_positive_predictions(clingo_output, rule_numbers):\n    # rule_predict_pos<number>(<warning>)\n    positive_predictions = []\n    for line in clingo_output:\n        if line.startswith('rule_predict_pos'):\n            number_str = line.split('(')[0].split('rule_predict_pos')[1]\n            number = int(number_str) if number_str.isdigit() else 0\n            if number not in rule_numbers:\n                continue\n            # extract warning from parenthesis\n            warning = line.split('(')[1].split(')')[0]\n            positive_predictions.append(warning)\n    return positive_predictions\n\ndef get_positive_predictions_of_rule(clingo_output, rule_number):\n    # rule_predict_pos<number>(<warning>)\n    positive_predictions = []\n    for line in clingo_output:\n        matches_rule = line.startswith(f'rule_predict_pos{rule_number}') if rule_number != 0 else line.startswith('rule_predict_pos')\n        if matches_rule:\n            # extract warning from parenthesis\n            warning = line.split('(')[1].split(')')[0]\n            positive_predictions.append(warning)\n    return positive_predictions\n\n# Simulation driver code\n\n# Parameters for the simulation\nwarning_type = sys.argv[1]\nground_truth_file = sys.argv[2]\n\n# Read ground_truth data\nground_truth = read_ground_truth(ground_truth_file)\n\ngraph_id_to_warning = {}\nwith open('meteor_app/private/original_graphs/' + warning_type + '_graph_id_mapping.txt', 'r') as file:\n    for line in file:\n        graph_id = line.split(',')[1]\n        example_id = line.split(',')[2].split(' - ')[0]\n        graph_id_to_warning[graph_id] = example_id\n\n# Load code data from JSON\nwith open('meteor_app/private/original_graphs/' + warning_type + '_elementpositions.json', 'r') as file:\n    code_content_data = json.load(file)\n\ncode_data = {}\n\nfor graph_id_str, another_json_str in code_content_data.items():\n    warning_data = json.loads(another_json_str)\n\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n    try:\n        warning_id = int(graph_id_to_warning[graph_id_str])\n    except:\n        continue\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(range(len(accuracy_list)), accuracy_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(range(len(rule_percentage_list)), rule_percentage_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(range(len(conciseness_list)), conciseness_list, label=f'{scenario_name} - {p_value}', marker='o')\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:51.879Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['398','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:19:51.879Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['415','0','423','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:19:51.879Z'},{src:'onDidChangeActiveTextEditor',msg:'Current editor: [%s]; Previous editor: [%s]',prm:['','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-05T08:19:52.558Z'},{src:'onDidChangeActiveTextEditor',msg:'Current editor: [%s]; Previous editor: [%s]',prm:['untitled:Untitled-1',''],time:'2024-09-05T08:19:52.575Z'},{src:'onDidChangeTextDocument',msg:'%s:%s to %s:%s in [%s] replaced with: %s`',prm:['0','0','0','0','Untitled-1',"# the idea is to try different combinations \n# IMPORTANT: prequisites for running this script : run `meteor` in the `meteor_app` directory \n# Run one of \n# e.g., WARNING_TYPE=apache_lucene-solr__NULL_ WARNING_JSON_NAME=spotbugs_warnings_apache_lucene-solr__NULL_ meteor  \n# e.g., WARNING_JSON_NAME=infer_warnings_alibaba_nacos_NULL_DEREFERENCE meteor \n# e.g,, WARNING_TYPE=RESOURCE_LEAK__presto WARNING_JSON_NAME=infer_warnings_prestodb_presto_RESOURCE_LEAK meteor \n# and generate the background.lp file \n\nimport random\nimport subprocess\nimport sys\nimport os\nfrom collections import defaultdict\nimport json\nimport time\nimport matplotlib.pyplot as plt\n\ndef read_ground_truth(ground_truth_file):\n    ground_truth = {}\n    with open(ground_truth_file) as f:\n        for line in f:\n            line = line.strip().split()\n            ground_truth[line[0]] = line[1]\n    return ground_truth\n\ndef write_labels_to_clingo_input(poss, negs):\n    with open('lp/simulation_labels.lp', 'w+') as f:\n        for p in poss:\n            f.write(f'pos({p}).\\n')\n        for n in negs:\n            f.write(f'neg({n}).\\n')\n\ndef read_containment(containment_file='lp/background.lp'):\n    containment = {}\n    last_checked_id = -1  # Initialize last_checked_id to keep track of the last ID we processed\n\n    with open(containment_file) as f:\n        for line in f:\n            line = line.strip().split()\n\n            # If line contains 'containment', extract the warning ID\n            if 'containment' in line[0]:\n                id = int(line[0].split('containment(')[1].split(',')[0])\n\n                # Only process the first occurrence of each ID\n                if id != last_checked_id:\n                    loc = line[1].split(\")\")[0]\n                    loc = loc.replace('__', '.').replace('_', '.')\n                    last_checked_id = id  # Update last_checked_id to prevent re-checking this ID\n                    containment[id] = loc  # Store the location for this ID\n    return containment\n\ndef heuristic_shorter_code_first(warnings):\n    \"\"\"Sort the warnings by lines of code (ascending).\"\"\"\n    return sorted(warnings, key=lambda x: x[1])\n\ndef heuristic_shared_function_calls(warnings):\n    \"\"\"Sort the warnings by the number of shared function calls (descending).\"\"\"\n    def shared_function_calls(warning, other_warnings):\n        return sum(len(set(warning[2]).intersection(set(other[2]))) for other in other_warnings)\n    #print(sorted(warnings, key=lambda x: shared_function_calls(x, warnings), reverse=True))\n    return sorted(warnings, key=lambda x: shared_function_calls(x, warnings), reverse=True)\n\ndef heuristic_neighbor_classes(warnings, containment):\n    \"\"\"Sort the warnings by neighbor classes (contained in the same package or directory).\"\"\"\n    sorted_warnings = []\n    processed_locs = set()\n    \n    for i in range(len(warnings)):\n        loc = containment.get(int(warnings[i][0]))\n        if loc not in processed_locs:\n            # Find all warnings with the same location\n            same_loc_warnings = [warnings[j] for j in range(i, len(warnings)) if containment.get(int(warnings[j][0])) == loc]\n            sorted_warnings.extend(same_loc_warnings)\n            processed_locs.add(loc)\n    \n    return sorted_warnings\n\ndef initialize_warnings_state(ground_truth):\n    \"\"\"Initialize all warnings with the state 'uninspected'.\"\"\"\n    return {k: 'uninspected' for k in ground_truth.keys()}\n\ndef calculate_accuracy(warnings_state, ground_truth):\n    \"\"\"Calculate the accuracy by comparing the current state of warnings with the ground truth.\"\"\"\n    correct_labels = sum(1 for k, v in warnings_state.items() if v == ground_truth[k])\n    return correct_labels / len(ground_truth) * 100\n\ndef sample_labels_randomized_then_sorted(ground_truth, num_pos, num_neg, code_data, warnings_state, apply_heuristics=None):\n    \"\"\"\n    Apply specified heuristics to the warnings before sampling:\n    - apply_heuristics: A list containing the numbers [1, 2, 3] corresponding to the heuristics to apply.\n        1: Review the shorter code first.\n        2: Look for similar code (shared API calls).\n        3: Look for neighbor classes (contained in the same package or directory).\n    - After applying heuristics, select either one positive or one negative warning based on a coin toss.\n    \"\"\"\n    if apply_heuristics is None:\n        apply_heuristics = [1, 2, 3]  # Default to applying all heuristics\n    \n    # Get all uninspected warnings with their lines of code and function calls\n    warnings = [\n        (k, code_data.get(int(k), {}).get('linesOfCode', 0), code_data.get(int(k), {}).get('functionCalls', []))\n        for k, v in ground_truth.items() if warnings_state[k] == 'uninspected'\n    ]\n    \n    if not warnings:  # If all warnings have been inspected, return empty lists\n        return [], []\n    \n    # Apply heuristics in the specified order\n    containment = read_containment()\n    for heuristic in apply_heuristics:\n        if heuristic == 1:\n            warnings = heuristic_shorter_code_first(warnings)\n        elif heuristic == 2:\n            warnings = heuristic_shared_function_calls(warnings)\n        elif heuristic == 3:\n            warnings = heuristic_neighbor_classes(warnings, containment)\n\n    # Sort by code length to identify shortest and longest warnings\n    sorted_warnings_by_length = sorted(warnings, key=lambda x: x[1])\n    \n    # Coin toss to decide whether to pick a positive or negative warning\n    coin_toss = random.choice(['positive', 'negative'])\n    \n    if coin_toss == 'positive':\n        # Select the shortest warning for positive\n        selected_positive_warnings = [sorted_warnings_by_length[0][0]]\n        selected_negative_warnings = []\n        warnings_state[selected_positive_warnings[0]] = 'positive'\n        #print('Coin toss result: Positive selected')\n    else:\n        # Select the longest warning for negative\n        selected_positive_warnings = []\n        selected_negative_warnings = [sorted_warnings_by_length[-1][0]]\n        warnings_state[selected_negative_warnings[0]] = 'negative'\n        #print('Coin toss result: Negative selected')\n    \n    print('Selected positive warning:', selected_positive_warnings)\n    print('Selected negative warning:', selected_negative_warnings)\n    \n    return selected_positive_warnings, selected_negative_warnings\n\n\ndef run_clingo():\n    files = [\n        f'lp/simulation_labels.lp',  \n        'lp/background.lp',              \n        'lp/frozen_rules.lp',            \n        'lp/rules2.lp'                   \n    ]\n    \n    command = ['clingo'] + files + ['--outf=2'] + ['--time-limit=30'] \n    print(' '.join(command))\n    result = subprocess.run(command, capture_output=True, text=True)\n\n    return result.stdout\n\ndef parse_clingo_output(data):\n    # Load the data using json.loads if 'data' is a string,\n    # otherwise assume it's already a dictionary\n    if isinstance(data, str):\n        data = json.loads(data)\n    \n    # Navigate through the JSON structure\n    # Assuming 'Call' is always present and has at least one element\n    calls = data.get(\"Call\", [])\n    if calls:\n        # Assuming 'Witnesses' is always present in the last element of 'Call' and has at least one element\n        last_call = calls[-1]\n        witnesses = last_call.get(\"Witnesses\", [])\n        if witnesses:\n            # Get the last 'Witnesses' entry\n            last_witness = witnesses[-1]\n            # Return the 'Value' list from the last 'Witnesses' entry\n            return last_witness.get(\"Value\", [])\n    return []\n\ndef extract_summary_rules(clingo_output):\n    # a summary rule is one prefixed by rule_contains(number)\n    summary_rules_by_prefix = defaultdict(list)\n    for line in clingo_output:\n        if line.startswith('rule_contains'):\n\n            number_str = line.split('(')[0].split('rule_contains')[1]\n            number = int(number_str) if number_str.isdigit() else 0\n\n            rule = line.split('(')[1].split(')')[0]\n            summary_rules_by_prefix[number].append(rule)\n    return summary_rules_by_prefix\n\ndef calculate_rule_percentage(clingo_output, positive_predictions):\n    summary_rules_by_prefix = defaultdict(list)\n    rule_percentages = {}\n    for line in clingo_output:\n        if line.startswith('rule_predict_pos'):\n            number_str = line.split('(')[0].split('rule_predict_pos')[1]\n            if number_str == '':\n                number_str = '0'\n            number = int(number_str)\n            warning_number = line.split('(')[1].split(')')[0]\n            summary_rules_by_prefix[number].append(warning_number)\n    for rule_number, matched_warnings in summary_rules_by_prefix.items():\n        rule_percentages[rule_number] = len(set(matched_warnings) & set(positive_predictions)) / len(set(matched_warnings))\n    return rule_percentages\n\ndef number_of_rules_over_percentage(percentages, percentage_threshold=0.8):\n    return sum(1 for p in percentages.values() if p >= percentage_threshold)\n\ndef get_number_of_positive_predictions(clingo_output):\n    positive_predictions = set()\n    for line in clingo_output:\n        if line.startswith('rule_predict_pos'):\n            warning_number = line.split('(')[1].split(')')[0]\n            positive_predictions.add(warning_number)\n    return len(positive_predictions)\n\n\ndef get_positive_predictions(clingo_output, rule_numbers):\n    # rule_predict_pos<number>(<warning>)\n    positive_predictions = []\n    for line in clingo_output:\n        if line.startswith('rule_predict_pos'):\n            number_str = line.split('(')[0].split('rule_predict_pos')[1]\n            number = int(number_str) if number_str.isdigit() else 0\n            if number not in rule_numbers:\n                continue\n            # extract warning from parenthesis\n            warning = line.split('(')[1].split(')')[0]\n            positive_predictions.append(warning)\n    return positive_predictions\n\ndef get_positive_predictions_of_rule(clingo_output, rule_number):\n    # rule_predict_pos<number>(<warning>)\n    positive_predictions = []\n    for line in clingo_output:\n        matches_rule = line.startswith(f'rule_predict_pos{rule_number}') if rule_number != 0 else line.startswith('rule_predict_pos')\n        if matches_rule:\n            # extract warning from parenthesis\n            warning = line.split('(')[1].split(')')[0]\n            positive_predictions.append(warning)\n    return positive_predictions\n\n# Simulation driver code\n\n# Parameters for the simulation\nwarning_type = sys.argv[1]\nground_truth_file = sys.argv[2]\n\n# Read ground_truth data\nground_truth = read_ground_truth(ground_truth_file)\n\ngraph_id_to_warning = {}\nwith open('meteor_app/private/original_graphs/' + warning_type + '_graph_id_mapping.txt', 'r') as file:\n    for line in file:\n        graph_id = line.split(',')[1]\n        example_id = line.split(',')[2].split(' - ')[0]\n        graph_id_to_warning[graph_id] = example_id\n\n# Load code data from JSON\nwith open('meteor_app/private/original_graphs/' + warning_type + '_elementpositions.json', 'r') as file:\n    code_content_data = json.load(file)\n\ncode_data = {}\n\nfor graph_id_str, another_json_str in code_content_data.items():\n    warning_data = json.loads(another_json_str)\n\n    # Code length\n    raw_code = warning_data.get(\"rawCode\", \"\")\n    raw_code_length = len(raw_code.splitlines())\n\n    warning_id = int(graph_id_to_warning[graph_id_str])\n\n    code_data[warning_id] = {}\n\n    code_data[warning_id]['linesOfCode'] = raw_code_length\n\n    # Find all code expressions that look like function calls\n    function_calls = [expression for expression in warning_data['expressionStart'].keys() if '()' in expression and '->' not in expression]\n\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n"],time:'2024-09-05T08:19:53.430Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['0','0','27','22','Untitled-1'],time:'2024-09-05T08:19:53.431Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['468','0','468','0','Untitled-1',''],time:'2024-09-05T08:19:53.439Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['442','0','468','0','Untitled-1'],time:'2024-09-05T08:19:53.439Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['450','0','468','0','Untitled-1'],time:'2024-09-05T08:19:53.890Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['442','0','468','0','Untitled-1'],time:'2024-09-05T08:19:54.490Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['467','0','467','0','Untitled-1',''],time:'2024-09-05T08:19:55.767Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['466','10','467','0','Untitled-1','\n'],time:'2024-09-05T08:19:55.924Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['465','11','467','0','Untitled-1','ue)\nplt.show()\n'],time:'2024-09-05T08:19:55.974Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['464','9','467','0','Untitled-1','d()\nplt.grid(True)\nplt.show()\n'],time:'2024-09-05T08:19:55.991Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['464','7','467','0','Untitled-1','end()\nplt.grid(True)\nplt.show()\n'],time:'2024-09-05T08:19:56.007Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['463','5','467','0','Untitled-1',"itle('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:56.029Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['460','0','467','0','Untitled-1',"\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:56.041Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['459','1','467','0','Untitled-1',"       )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:56.056Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['458','0','467','0','Untitled-1',"            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:56.074Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['457','0','467','0','Untitled-1',"            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:56.107Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['456','0','467','0','Untitled-1',"            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:56.124Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['455','0','467','0','Untitled-1',"            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:56.140Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['454','0','467','0','Untitled-1',"            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:56.156Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['452','0','467','0','Untitled-1',"        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:56.191Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['450','0','467','0','Untitled-1',"for scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:56.206Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['449','0','467','0','Untitled-1',"plt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:56.240Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['448','0','467','0','Untitled-1',"# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:56.273Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['447','0','467','0','Untitled-1',"\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:56.290Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['446','0','467','0','Untitled-1',"plt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:56.339Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['445','0','467','0','Untitled-1',"plt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:56.389Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['443','0','467','0','Untitled-1',"plt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:56.473Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['442','0','467','0','Untitled-1',"plt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:56.506Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['441','0','467','0','Untitled-1',"plt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:56.557Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['441','0','468','0','Untitled-1'],time:'2024-09-05T08:19:56.557Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['441','0','467','0','Untitled-1'],time:'2024-09-05T08:19:56.623Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['440','0','467','0','Untitled-1',"\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:56.628Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['440','0','467','0','Untitled-1'],time:'2024-09-05T08:19:56.639Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['439','0','467','0','Untitled-1',"        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:56.642Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['439','0','466','10','Untitled-1'],time:'2024-09-05T08:19:56.656Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['438','0','467','0','Untitled-1',"            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:56.659Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['438','0','465','14','Untitled-1'],time:'2024-09-05T08:19:56.672Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['437','0','467','0','Untitled-1',"            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:56.675Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['437','0','464','12','Untitled-1'],time:'2024-09-05T08:19:56.689Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['436','0','467','0','Untitled-1',"            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:56.692Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['436','0','463','82','Untitled-1'],time:'2024-09-05T08:19:56.706Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['435','0','467','0','Untitled-1',"            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:56.709Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['435','0','462','25','Untitled-1'],time:'2024-09-05T08:19:56.722Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['434','0','467','0','Untitled-1',"            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:56.725Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['434','0','461','30','Untitled-1'],time:'2024-09-05T08:19:56.739Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['433','0','467','0','Untitled-1',"            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:56.742Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['433','0','460','0','Untitled-1'],time:'2024-09-05T08:19:56.756Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['433','0','459','9','Untitled-1'],time:'2024-09-05T08:19:56.772Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['432','0','467','0','Untitled-1',"        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:56.775Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['432','0','458','22','Untitled-1'],time:'2024-09-05T08:19:56.789Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['431','0','467','0','Untitled-1',"    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:56.792Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['431','0','457','46','Untitled-1'],time:'2024-09-05T08:19:56.806Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['430','0','467','0','Untitled-1',"for scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:56.809Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['430','0','456','47','Untitled-1'],time:'2024-09-05T08:19:56.822Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['429','0','467','0','Untitled-1',"plt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:56.825Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['429','0','456','47','Untitled-1'],time:'2024-09-05T08:19:56.838Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['428','0','467','0','Untitled-1',"# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:56.841Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['428','0','455','67','Untitled-1'],time:'2024-09-05T08:19:56.856Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['427','0','467','0','Untitled-1',"\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:56.858Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['427','0','454','30','Untitled-1'],time:'2024-09-05T08:19:56.872Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['426','0','467','0','Untitled-1',"plt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:56.875Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['426','0','453','42','Untitled-1'],time:'2024-09-05T08:19:56.888Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['425','0','467','0','Untitled-1',"plt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:56.891Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['425','0','452','17','Untitled-1'],time:'2024-09-05T08:19:56.906Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['424','0','467','0','Untitled-1',"plt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:56.908Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['424','0','451','55','Untitled-1'],time:'2024-09-05T08:19:56.922Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['423','0','467','0','Untitled-1',"plt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:56.925Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['423','0','450','60','Untitled-1'],time:'2024-09-05T08:19:56.939Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['422','0','467','0','Untitled-1',"plt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:56.942Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['422','0','449','27','Untitled-1'],time:'2024-09-05T08:19:56.956Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['421','0','467','0','Untitled-1',"plt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:56.958Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['421','0','448','68','Untitled-1'],time:'2024-09-05T08:19:56.972Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['420','0','467','0','Untitled-1',"\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:56.975Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['420','0','447','0','Untitled-1'],time:'2024-09-05T08:19:56.989Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['420','0','446','10','Untitled-1'],time:'2024-09-05T08:19:57.006Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['419','0','467','0','Untitled-1',"        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:57.008Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['419','0','445','14','Untitled-1'],time:'2024-09-05T08:19:57.022Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['418','0','467','0','Untitled-1',"            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:57.025Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['418','0','444','12','Untitled-1'],time:'2024-09-05T08:19:57.039Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['417','0','467','0','Untitled-1',"            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:57.042Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['417','0','443','105','Untitled-1'],time:'2024-09-05T08:19:57.056Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['416','0','467','0','Untitled-1',"            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:57.058Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['416','0','443','105','Untitled-1'],time:'2024-09-05T08:19:57.072Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['415','0','467','0','Untitled-1',"            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:57.075Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['415','0','442','52','Untitled-1'],time:'2024-09-05T08:19:57.089Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['414','0','467','0','Untitled-1',"            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:57.092Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['414','0','441','30','Untitled-1'],time:'2024-09-05T08:19:57.106Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['413','0','467','0','Untitled-1',"            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:57.108Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['413','0','440','0','Untitled-1'],time:'2024-09-05T08:19:57.122Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['412','0','467','0','Untitled-1',"        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:57.125Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['412','0','439','9','Untitled-1'],time:'2024-09-05T08:19:57.139Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['411','0','467','0','Untitled-1',"    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:57.142Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['411','0','438','22','Untitled-1'],time:'2024-09-05T08:19:57.156Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['410','0','467','0','Untitled-1',"for scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:57.158Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['410','0','437','46','Untitled-1'],time:'2024-09-05T08:19:57.172Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['409','0','467','0','Untitled-1',"plt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:57.175Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['409','0','436','47','Untitled-1'],time:'2024-09-05T08:19:57.189Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['408','0','467','0','Untitled-1',"# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:57.192Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['408','0','435','67','Untitled-1'],time:'2024-09-05T08:19:57.206Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['407','0','467','0','Untitled-1',"\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:57.209Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['408','0','434','34','Untitled-1'],time:'2024-09-05T08:19:57.222Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['407','0','433','46','Untitled-1'],time:'2024-09-05T08:19:57.239Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['406','0','467','0','Untitled-1',"}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:57.242Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['406','0','432','17','Untitled-1'],time:'2024-09-05T08:19:57.256Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['405','0','467','0','Untitled-1',"    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:57.258Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['405','0','431','59','Untitled-1'],time:'2024-09-05T08:19:57.272Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['404','0','467','0','Untitled-1',"    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:57.275Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['404','0','431','59','Untitled-1'],time:'2024-09-05T08:19:57.289Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['403','0','467','0','Untitled-1',"    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:57.292Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['403','0','430','64','Untitled-1'],time:'2024-09-05T08:19:57.306Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['402','0','467','0','Untitled-1',"    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:57.309Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['403','0','429','27','Untitled-1'],time:'2024-09-05T08:19:57.323Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['402','0','429','27','Untitled-1'],time:'2024-09-05T08:19:57.339Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['401','0','467','0','Untitled-1',"heuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:57.341Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['402','0','467','0','Untitled-1',"    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:57.356Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['403','0','467','0','Untitled-1',"    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:57.406Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['402','0','467','0','Untitled-1',"    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:57.625Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['401','0','467','0','Untitled-1',"heuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:57.690Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['401','0','428','91','Untitled-1'],time:'2024-09-05T08:19:57.706Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['400','0','467','0','Untitled-1',"# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:57.710Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['401','0','427','0','Untitled-1'],time:'2024-09-05T08:19:57.723Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['400','0','427','0','Untitled-1'],time:'2024-09-05T08:19:57.739Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['399','0','467','0','Untitled-1',"\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:57.743Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['399','0','426','10','Untitled-1'],time:'2024-09-05T08:19:57.756Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['398','0','467','0','Untitled-1',"}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:57.759Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['399','0','425','14','Untitled-1'],time:'2024-09-05T08:19:57.772Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['398','0','425','14','Untitled-1'],time:'2024-09-05T08:19:57.789Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['397','0','467','0','Untitled-1',"    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:57.792Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['397','0','424','12','Untitled-1'],time:'2024-09-05T08:19:57.806Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['396','0','467','0','Untitled-1',"    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:57.809Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['396','0','423','79','Untitled-1'],time:'2024-09-05T08:19:57.822Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['396','0','422','26','Untitled-1'],time:'2024-09-05T08:19:57.839Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['395','0','467','0','Untitled-1',"    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:57.842Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['395','0','422','26','Untitled-1'],time:'2024-09-05T08:19:57.856Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['394','0','467','0','Untitled-1',"texture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:57.858Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['394','0','421','30','Untitled-1'],time:'2024-09-05T08:19:57.872Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['393','0','467','0','Untitled-1',"# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:57.875Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['394','0','420','0','Untitled-1'],time:'2024-09-05T08:19:57.889Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['393','0','420','0','Untitled-1'],time:'2024-09-05T08:19:57.906Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['392','0','467','0','Untitled-1',"\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:57.908Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['392','0','419','9','Untitled-1'],time:'2024-09-05T08:19:57.922Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['391','0','467','0','Untitled-1',"}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:57.925Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['392','0','418','22','Untitled-1'],time:'2024-09-05T08:19:57.939Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['391','0','417','46','Untitled-1'],time:'2024-09-05T08:19:57.956Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['390','0','467','0','Untitled-1',"    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:57.959Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['390','0','417','46','Untitled-1'],time:'2024-09-05T08:19:57.972Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['389','0','467','0','Untitled-1',"    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:57.975Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['389','0','416','47','Untitled-1'],time:'2024-09-05T08:19:57.989Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['388','0','467','0','Untitled-1',"    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:57.992Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['389','0','415','67','Untitled-1'],time:'2024-09-05T08:19:58.006Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['388','0','415','67','Untitled-1'],time:'2024-09-05T08:19:58.022Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['387','0','467','0','Untitled-1',"    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:58.025Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['388','0','414','27','Untitled-1'],time:'2024-09-05T08:19:58.039Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['387','0','414','27','Untitled-1'],time:'2024-09-05T08:19:58.056Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['386','0','467','0','Untitled-1',"color_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:58.058Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['387','0','413','39','Untitled-1'],time:'2024-09-05T08:19:58.072Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['386','0','413','39','Untitled-1'],time:'2024-09-05T08:19:58.089Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['385','0','467','0','Untitled-1',"# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:58.091Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['385','0','412','17','Untitled-1'],time:'2024-09-05T08:19:58.106Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['384','0','467','0','Untitled-1',"\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:58.108Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['384','0','411','52','Untitled-1'],time:'2024-09-05T08:19:58.139Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['383','0','467','0','Untitled-1',"\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:58.141Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['384','0','410','57','Untitled-1'],time:'2024-09-05T08:19:58.156Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['383','0','410','57','Untitled-1'],time:'2024-09-05T08:19:58.172Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['382','0','467','0','Untitled-1',"            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:58.175Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['383','0','409','27','Untitled-1'],time:'2024-09-05T08:19:58.189Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['382','0','409','27','Untitled-1'],time:'2024-09-05T08:19:58.206Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['381','0','467','0','Untitled-1',"            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:58.209Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['382','0','408','65','Untitled-1'],time:'2024-09-05T08:19:58.222Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['381','0','408','65','Untitled-1'],time:'2024-09-05T08:19:58.239Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['380','0','467','0','Untitled-1',"            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:58.242Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['380','0','407','0','Untitled-1'],time:'2024-09-05T08:19:58.256Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['379','0','467','0','Untitled-1',"            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:58.260Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['379','0','406','1','Untitled-1'],time:'2024-09-05T08:19:58.289Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['378','0','467','0','Untitled-1',"            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:58.292Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['379','0','405','32','Untitled-1'],time:'2024-09-05T08:19:58.307Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['378','0','405','32','Untitled-1'],time:'2024-09-05T08:19:58.322Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['377','0','467','0','Untitled-1',"            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:58.325Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['377','0','404','43','Untitled-1'],time:'2024-09-05T08:19:58.346Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['376','0','467','0','Untitled-1',"            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:58.356Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['377','0','403','42','Untitled-1'],time:'2024-09-05T08:19:58.372Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['376','0','403','42','Untitled-1'],time:'2024-09-05T08:19:58.388Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['375','0','467','0','Untitled-1',"\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:58.391Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['375','0','402','38','Untitled-1'],time:'2024-09-05T08:19:58.405Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['374','0','467','0','Untitled-1',"                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:58.408Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['374','0','401','19','Untitled-1'],time:'2024-09-05T08:19:58.439Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['373','0','467','0','Untitled-1',"                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:58.442Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['374','0','400','25','Untitled-1'],time:'2024-09-05T08:19:58.456Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['373','0','400','25','Untitled-1'],time:'2024-09-05T08:19:58.472Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['372','0','467','0','Untitled-1',"\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:58.475Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['373','0','399','0','Untitled-1'],time:'2024-09-05T08:19:58.489Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['372','0','399','0','Untitled-1'],time:'2024-09-05T08:19:58.506Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['371','0','467','0','Untitled-1',"                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:58.508Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['372','0','398','1','Untitled-1'],time:'2024-09-05T08:19:58.522Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['371','0','398','1','Untitled-1'],time:'2024-09-05T08:19:58.539Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['370','0','467','0','Untitled-1',"                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:58.542Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['370','0','397','19','Untitled-1'],time:'2024-09-05T08:19:58.556Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['369','0','467','0','Untitled-1',"                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:58.559Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['369','0','396','22','Untitled-1'],time:'2024-09-05T08:19:58.589Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['368','0','467','0','Untitled-1',"                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:58.592Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['369','0','395','19','Untitled-1'],time:'2024-09-05T08:19:58.606Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['368','0','395','19','Untitled-1'],time:'2024-09-05T08:19:58.622Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['367','0','467','0','Untitled-1',"                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:58.625Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['368','0','394','18','Untitled-1'],time:'2024-09-05T08:19:58.639Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['367','0','394','18','Untitled-1'],time:'2024-09-05T08:19:58.656Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['366','0','467','0','Untitled-1',"\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:58.658Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['367','0','393','38','Untitled-1'],time:'2024-09-05T08:19:58.672Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['366','0','393','38','Untitled-1'],time:'2024-09-05T08:19:58.689Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['365','0','467','0','Untitled-1',"                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:58.692Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['365','0','392','0','Untitled-1'],time:'2024-09-05T08:19:58.706Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['364','0','467','0','Untitled-1',"                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:58.709Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['364','0','391','1','Untitled-1'],time:'2024-09-05T08:19:58.739Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['363','0','467','0','Untitled-1',"                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:58.742Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['364','0','390','46','Untitled-1'],time:'2024-09-05T08:19:58.756Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['363','0','390','46','Untitled-1'],time:'2024-09-05T08:19:58.772Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['362','0','467','0','Untitled-1',"                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:58.775Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['363','0','389','54','Untitled-1'],time:'2024-09-05T08:19:58.789Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['362','0','389','54','Untitled-1'],time:'2024-09-05T08:19:58.806Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['361','0','467','0','Untitled-1',"\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:58.808Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['362','0','388','53','Untitled-1'],time:'2024-09-05T08:19:58.822Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['361','0','388','53','Untitled-1'],time:'2024-09-05T08:19:58.839Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['360','0','467','0','Untitled-1',"                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:58.842Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['360','0','387','49','Untitled-1'],time:'2024-09-05T08:19:58.856Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['359','0','467','0','Untitled-1',"                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:58.859Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['359','0','386','16','Untitled-1'],time:'2024-09-05T08:19:58.889Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['358','0','467','0','Untitled-1',"                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:58.892Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['359','0','385','44','Untitled-1'],time:'2024-09-05T08:19:58.906Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['358','0','385','44','Untitled-1'],time:'2024-09-05T08:19:58.922Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['357','0','467','0','Untitled-1',"                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:58.926Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['358','0','384','0','Untitled-1'],time:'2024-09-05T08:19:58.939Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['357','0','384','0','Untitled-1'],time:'2024-09-05T08:19:58.955Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['356','0','467','0','Untitled-1',"                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:58.958Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['357','0','383','0','Untitled-1'],time:'2024-09-05T08:19:58.971Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['356','0','383','0','Untitled-1'],time:'2024-09-05T08:19:58.989Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['355','0','467','0','Untitled-1',"                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:58.992Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['355','0','382','80','Untitled-1'],time:'2024-09-05T08:19:59.006Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['354','0','467','0','Untitled-1',"            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:59.009Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['355','0','381','108','Untitled-1'],time:'2024-09-05T08:19:59.023Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['354','0','381','108','Untitled-1'],time:'2024-09-05T08:19:59.039Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['353','0','467','0','Untitled-1',"\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:59.042Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['354','0','380','75','Untitled-1'],time:'2024-09-05T08:19:59.056Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['353','0','380','75','Untitled-1'],time:'2024-09-05T08:19:59.072Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['352','0','467','0','Untitled-1',"            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:59.075Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['353','0','379','76','Untitled-1'],time:'2024-09-05T08:19:59.089Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['352','0','379','76','Untitled-1'],time:'2024-09-05T08:19:59.106Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['351','0','467','0','Untitled-1',"            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:59.109Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['351','0','378','71','Untitled-1'],time:'2024-09-05T08:19:59.122Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['350','0','467','0','Untitled-1',"            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:59.142Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['350','0','377','33','Untitled-1'],time:'2024-09-05T08:19:59.156Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['349','0','467','0','Untitled-1',"            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:59.159Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['350','0','376','53','Untitled-1'],time:'2024-09-05T08:19:59.172Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['349','0','376','53','Untitled-1'],time:'2024-09-05T08:19:59.189Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['348','0','467','0','Untitled-1',"            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:59.192Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['349','0','375','0','Untitled-1'],time:'2024-09-05T08:19:59.206Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['348','0','375','0','Untitled-1'],time:'2024-09-05T08:19:59.222Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['347','0','467','0','Untitled-1',"\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:59.225Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['348','0','374','86','Untitled-1'],time:'2024-09-05T08:19:59.239Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['347','0','374','86','Untitled-1'],time:'2024-09-05T08:19:59.256Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['346','0','467','0','Untitled-1',"                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:59.259Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['346','0','373','94','Untitled-1'],time:'2024-09-05T08:19:59.272Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['345','0','467','0','Untitled-1',"                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:59.292Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['345','0','372','0','Untitled-1'],time:'2024-09-05T08:19:59.306Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['344','0','467','0','Untitled-1',"            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:59.309Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['345','0','371','39','Untitled-1'],time:'2024-09-05T08:19:59.322Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['344','0','371','39','Untitled-1'],time:'2024-09-05T08:19:59.339Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['343','0','467','0','Untitled-1',"\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:59.342Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['344','0','370','21','Untitled-1'],time:'2024-09-05T08:19:59.356Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['343','0','370','21','Untitled-1'],time:'2024-09-05T08:19:59.372Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['342','0','467','0','Untitled-1',"                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:59.376Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['343','0','369','82','Untitled-1'],time:'2024-09-05T08:19:59.389Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['342','0','369','82','Untitled-1'],time:'2024-09-05T08:19:59.405Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['341','0','467','0','Untitled-1',"                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:59.409Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['341','0','368','33','Untitled-1'],time:'2024-09-05T08:19:59.423Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['340','0','467','0','Untitled-1',"                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:59.442Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['340','0','367','70','Untitled-1'],time:'2024-09-05T08:19:59.456Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['339','0','467','0','Untitled-1',"                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:59.459Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['340','0','366','0','Untitled-1'],time:'2024-09-05T08:19:59.472Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['339','0','366','0','Untitled-1'],time:'2024-09-05T08:19:59.489Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['338','0','467','0','Untitled-1',"                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:59.493Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['339','0','365','35','Untitled-1'],time:'2024-09-05T08:19:59.506Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['338','0','365','35','Untitled-1'],time:'2024-09-05T08:19:59.522Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['337','0','467','0','Untitled-1',"                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:59.526Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['338','0','364','21','Untitled-1'],time:'2024-09-05T08:19:59.539Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['337','0','364','21','Untitled-1'],time:'2024-09-05T08:19:59.556Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['336','0','467','0','Untitled-1',"                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:59.559Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['336','0','363','91','Untitled-1'],time:'2024-09-05T08:19:59.572Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['335','0','467','0','Untitled-1',"                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:59.593Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['335','0','362','48','Untitled-1'],time:'2024-09-05T08:19:59.606Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['334','0','467','0','Untitled-1',"\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:59.609Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['335','0','361','0','Untitled-1'],time:'2024-09-05T08:19:59.623Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['334','0','361','0','Untitled-1'],time:'2024-09-05T08:19:59.639Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['333','0','467','0','Untitled-1',"                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:59.642Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['334','0','360','90','Untitled-1'],time:'2024-09-05T08:19:59.656Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['333','0','360','90','Untitled-1'],time:'2024-09-05T08:19:59.673Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['332','0','467','0','Untitled-1',"                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:59.676Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['333','0','359','47','Untitled-1'],time:'2024-09-05T08:19:59.689Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['332','0','359','47','Untitled-1'],time:'2024-09-05T08:19:59.706Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['331','0','467','0','Untitled-1',"                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:59.709Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['331','0','358','87','Untitled-1'],time:'2024-09-05T08:19:59.723Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['330','0','467','0','Untitled-1',"                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:59.743Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['330','0','357','74','Untitled-1'],time:'2024-09-05T08:19:59.756Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['329','0','467','0','Untitled-1',"                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:59.759Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['330','0','356','61','Untitled-1'],time:'2024-09-05T08:19:59.772Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['329','0','356','61','Untitled-1'],time:'2024-09-05T08:19:59.789Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['328','0','467','0','Untitled-1',"                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:59.792Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['329','0','355','51','Untitled-1'],time:'2024-09-05T08:19:59.806Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['328','0','355','51','Untitled-1'],time:'2024-09-05T08:19:59.822Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['327','0','467','0','Untitled-1',"                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:59.825Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['328','0','354','22','Untitled-1'],time:'2024-09-05T08:19:59.840Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['327','0','354','22','Untitled-1'],time:'2024-09-05T08:19:59.856Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['326','0','467','0','Untitled-1',"                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:59.860Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['326','0','353','0','Untitled-1'],time:'2024-09-05T08:19:59.880Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['325','0','467','0','Untitled-1',"                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:59.885Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['326','0','352','33','Untitled-1'],time:'2024-09-05T08:19:59.901Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['325','0','352','33','Untitled-1'],time:'2024-09-05T08:19:59.908Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['324','0','467','0','Untitled-1',"            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:59.911Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['324','0','351','64','Untitled-1'],time:'2024-09-05T08:19:59.939Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['323','0','467','0','Untitled-1',"\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:59.942Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['324','0','350','82','Untitled-1'],time:'2024-09-05T08:19:59.958Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['323','0','350','82','Untitled-1'],time:'2024-09-05T08:19:59.972Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['322','0','467','0','Untitled-1',"                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:19:59.975Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['323','0','349','82','Untitled-1'],time:'2024-09-05T08:19:59.989Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['322','0','349','82','Untitled-1'],time:'2024-09-05T08:20:00.006Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','467','0','Untitled-1',"                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:00.009Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['322','0','348','57','Untitled-1'],time:'2024-09-05T08:20:00.022Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['321','0','348','57','Untitled-1'],time:'2024-09-05T08:20:00.039Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['320','0','467','0','Untitled-1',"                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:00.043Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['320','0','347','0','Untitled-1'],time:'2024-09-05T08:20:00.056Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['319','0','467','0','Untitled-1',"                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:00.077Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['319','0','346','157','Untitled-1'],time:'2024-09-05T08:20:00.089Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['318','0','467','0','Untitled-1',"                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:00.093Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['319','0','345','69','Untitled-1'],time:'2024-09-05T08:20:00.106Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['318','0','345','69','Untitled-1'],time:'2024-09-05T08:20:00.122Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['317','0','467','0','Untitled-1',"                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:00.126Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['318','0','344','17','Untitled-1'],time:'2024-09-05T08:20:00.139Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['317','0','344','17','Untitled-1'],time:'2024-09-05T08:20:00.155Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['316','0','467','0','Untitled-1',"\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:00.159Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['317','0','343','0','Untitled-1'],time:'2024-09-05T08:20:00.172Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['316','0','343','0','Untitled-1'],time:'2024-09-05T08:20:00.189Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['315','0','467','0','Untitled-1',"                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:00.193Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['315','0','342','161','Untitled-1'],time:'2024-09-05T08:20:00.206Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['314','0','467','0','Untitled-1',"                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:00.226Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['314','0','341','21','Untitled-1'],time:'2024-09-05T08:20:00.239Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['313','0','467','0','Untitled-1',"                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:00.242Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['314','0','340','64','Untitled-1'],time:'2024-09-05T08:20:00.256Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['313','0','340','64','Untitled-1'],time:'2024-09-05T08:20:00.272Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['312','0','467','0','Untitled-1',"                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:00.276Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['313','0','339','52','Untitled-1'],time:'2024-09-05T08:20:00.288Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['312','0','339','52','Untitled-1'],time:'2024-09-05T08:20:00.306Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['311','0','467','0','Untitled-1',"                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:00.309Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['312','0','338','43','Untitled-1'],time:'2024-09-05T08:20:00.322Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['311','0','338','43','Untitled-1'],time:'2024-09-05T08:20:00.339Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['310','0','467','0','Untitled-1',"                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:00.342Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['310','0','337','92','Untitled-1'],time:'2024-09-05T08:20:00.356Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['309','0','467','0','Untitled-1',"                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:00.376Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['309','0','336','53','Untitled-1'],time:'2024-09-05T08:20:00.389Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['308','0','467','0','Untitled-1',"                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:00.392Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['309','0','335','96','Untitled-1'],time:'2024-09-05T08:20:00.406Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['308','0','335','96','Untitled-1'],time:'2024-09-05T08:20:00.422Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['307','0','467','0','Untitled-1',"            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:00.426Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['308','0','334','0','Untitled-1'],time:'2024-09-05T08:20:00.439Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['307','0','334','0','Untitled-1'],time:'2024-09-05T08:20:00.456Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['306','0','467','0','Untitled-1',"\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:00.459Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['307','0','333','89','Untitled-1'],time:'2024-09-05T08:20:00.473Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['306','0','333','89','Untitled-1'],time:'2024-09-05T08:20:00.489Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['305','0','467','0','Untitled-1',"            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:00.492Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['305','0','332','111','Untitled-1'],time:'2024-09-05T08:20:00.506Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['304','0','467','0','Untitled-1',"        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:00.509Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['304','0','331','40','Untitled-1'],time:'2024-09-05T08:20:00.538Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['303','0','467','0','Untitled-1',"        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:00.541Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['304','0','330','80','Untitled-1'],time:'2024-09-05T08:20:00.556Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['303','0','330','80','Untitled-1'],time:'2024-09-05T08:20:00.572Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['302','0','467','0','Untitled-1',"\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:00.575Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['303','0','329','59','Untitled-1'],time:'2024-09-05T08:20:00.589Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['302','0','329','59','Untitled-1'],time:'2024-09-05T08:20:00.606Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['301','0','467','0','Untitled-1',"        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:00.608Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['302','0','328','30','Untitled-1'],time:'2024-09-05T08:20:00.622Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['301','0','328','30','Untitled-1'],time:'2024-09-05T08:20:00.639Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['300','0','467','0','Untitled-1',"        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:00.642Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['300','0','327','41','Untitled-1'],time:'2024-09-05T08:20:00.656Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['299','0','467','0','Untitled-1',"\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:00.658Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['299','0','326','41','Untitled-1'],time:'2024-09-05T08:20:00.689Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['298','0','467','0','Untitled-1',"        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:00.692Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['299','0','325','86','Untitled-1'],time:'2024-09-05T08:20:00.706Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['298','0','325','86','Untitled-1'],time:'2024-09-05T08:20:00.722Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['297','0','467','0','Untitled-1',"    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:00.725Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['298','0','324','32','Untitled-1'],time:'2024-09-05T08:20:00.739Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['297','0','324','32','Untitled-1'],time:'2024-09-05T08:20:00.756Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['296','0','467','0','Untitled-1',"for scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:00.759Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['297','0','323','0','Untitled-1'],time:'2024-09-05T08:20:00.772Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['296','0','323','0','Untitled-1'],time:'2024-09-05T08:20:00.789Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['295','0','467','0','Untitled-1',"# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:00.792Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['295','0','322','60','Untitled-1'],time:'2024-09-05T08:20:00.805Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['294','0','467','0','Untitled-1',"\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:00.808Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['294','0','321','48','Untitled-1'],time:'2024-09-05T08:20:00.839Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['293','0','467','0','Untitled-1',"conciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:00.842Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['294','0','320','39','Untitled-1'],time:'2024-09-05T08:20:00.856Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['293','0','320','39','Untitled-1'],time:'2024-09-05T08:20:00.872Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['292','0','467','0','Untitled-1',"rule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:00.875Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['293','0','319','88','Untitled-1'],time:'2024-09-05T08:20:00.889Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['292','0','319','88','Untitled-1'],time:'2024-09-05T08:20:00.906Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['291','0','467','0','Untitled-1',"accuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:00.909Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['292','0','318','49','Untitled-1'],time:'2024-09-05T08:20:00.922Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['291','0','318','49','Untitled-1'],time:'2024-09-05T08:20:00.939Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['290','0','467','0','Untitled-1',"# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:00.942Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['290','0','317','92','Untitled-1'],time:'2024-09-05T08:20:00.956Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['289','0','467','0','Untitled-1',"\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:00.958Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['289','0','316','0','Untitled-1'],time:'2024-09-05T08:20:00.989Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['288','0','467','0','Untitled-1',"}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:00.992Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['289','0','315','85','Untitled-1'],time:'2024-09-05T08:20:01.006Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['288','0','315','85','Untitled-1'],time:'2024-09-05T08:20:01.022Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['287','0','467','0','Untitled-1',"    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:01.025Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['288','0','314','107','Untitled-1'],time:'2024-09-05T08:20:01.039Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['287','0','314','107','Untitled-1'],time:'2024-09-05T08:20:01.056Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['286','0','467','0','Untitled-1',"    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:01.059Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['287','0','313','36','Untitled-1'],time:'2024-09-05T08:20:01.072Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['286','0','313','36','Untitled-1'],time:'2024-09-05T08:20:01.089Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['285','0','467','0','Untitled-1',"    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:01.092Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['285','0','312','76','Untitled-1'],time:'2024-09-05T08:20:01.106Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['284','0','467','0','Untitled-1',"    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:01.109Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['284','0','311','55','Untitled-1'],time:'2024-09-05T08:20:01.139Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['283','0','467','0','Untitled-1',"scenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:01.142Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['284','0','310','26','Untitled-1'],time:'2024-09-05T08:20:01.156Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['283','0','310','26','Untitled-1'],time:'2024-09-05T08:20:01.172Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['282','0','467','0','Untitled-1',"# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:01.175Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['283','0','309','37','Untitled-1'],time:'2024-09-05T08:20:01.189Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['282','0','309','37','Untitled-1'],time:'2024-09-05T08:20:01.206Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['281','0','467','0','Untitled-1',"\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:01.208Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['282','0','308','68','Untitled-1'],time:'2024-09-05T08:20:01.222Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['281','0','308','68','Untitled-1'],time:'2024-09-05T08:20:01.239Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['280','0','467','0','Untitled-1',"    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:01.242Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['280','0','307','28','Untitled-1'],time:'2024-09-05T08:20:01.256Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['279','0','467','0','Untitled-1',"\n    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:01.259Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['280','0','467','0','Untitled-1',"    code_data[warning_id]['functionCalls'] = function_calls\n\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:01.275Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['281','0','467','0','Untitled-1',"\n# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:01.296Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['282','0','467','0','Untitled-1',"# Scenarios: 1 = Heuristic 1, 2 = Heuristic 2, 3 = Heuristic 3, 4 = All Heuristics\nscenarios = {\n    'Heuristic 1 Only': [1],\n    'Heuristic 2 Only': [2],\n    'Heuristic 3 Only': [3],\n    'All Heuristics': [1, 2, 3]\n}\n\n# Store the accuracy, rule percentage, and conciseness results for each scenario\naccuracy_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nrule_percentage_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\nconciseness_results = {key: {'p=0': [], 'p=0.5': [], 'p=1': []} for key in scenarios.keys()}\n\n# Run simulations for each scenario\nfor scenario_name, heuristics in scenarios.items():\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings):\n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                output = run_clingo()\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() < 0.1:\n                            selected_pos = get_positive_predictions(model, rule_numbers)\n                        # Update states\n                        for pos in selected_pos:\n                            warnings_state[pos] = 'positive'\n\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    output = run_clingo()\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() < 0.1:\n                                selected_pos = get_positive_predictions(model, rule_numbers)\n                            # Update states\n                            for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            print(warnings_state)\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n\n# Define the color scheme for each heuristic\ncolor_scheme = {\n    'Heuristic 1 Only': 'blue',     # Code Length\n    'Heuristic 2 Only': 'green',    # Code Similarity\n    'Heuristic 3 Only': 'orange',   # Same Containment\n    'All Heuristics': 'red'         # Combined\n}\n\n# Define the textures for each p-value\ntexture_scheme = {\n    'p=0': 'solid',\n    'p=0.5': 'dashed',\n    'p=1': 'dotted'\n}\n\n# Updated heuristic names\nheuristic_names = {\n    'Heuristic 1 Only': 'Code Length',\n    'Heuristic 2 Only': 'Code Similarity',\n    'Heuristic 3 Only': 'Same Containment',\n    'All Heuristics': 'Combined'\n}\n\n# Plot the accuracy over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in accuracy_results.items():\n    for p_value, accuracy_list in p_results.items():\n        plt.plot(\n            range(len(accuracy_list)), \n            accuracy_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Accuracy (%)')\nplt.title('Accuracy over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the percentage of rules over threshold over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in rule_percentage_results.items():\n    for p_value, rule_percentage_list in p_results.items():\n        plt.plot(\n            range(len(rule_percentage_list)), \n            rule_percentage_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Percentage of Rules Over Threshold (%)')\nplt.title('Percentage of Rules Over Threshold over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Plot the conciseness over time for all scenarios and probabilities\nplt.figure(figsize=(12, 6))\nfor scenario_name, p_results in conciseness_results.items():\n    for p_value, conciseness_list in p_results.items():\n        plt.plot(\n            range(len(conciseness_list)), \n            conciseness_list, \n            label=f'{heuristic_names[scenario_name]} - {p_value}', \n            color=color_scheme[scenario_name], \n            linestyle=texture_scheme[p_value],\n            marker='o'\n        )\n\nplt.xlabel('Iteration Number')\nplt.ylabel('Conciseness')\nplt.title('Conciseness over Iterations for Different Scenarios and Probabilities')\nplt.legend()\nplt.grid(True)\nplt.show()\n"],time:'2024-09-05T08:20:01.357Z'}]
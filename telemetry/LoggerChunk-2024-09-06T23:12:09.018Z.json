[{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','356','107','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))"],time:'2024-09-06T23:12:09.029Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['329','0','356','107','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.065Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','357','85','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed"],time:'2024-09-06T23:12:09.068Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['329','0','357','85','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.086Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','357','22','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                      "],time:'2024-09-06T23:12:09.088Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['330','0','357','85','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.103Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','358','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n"],time:'2024-09-06T23:12:09.108Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['330','0','358','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.120Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','359','92','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative"],time:'2024-09-06T23:12:09.124Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['331','0','358','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.136Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','359','22','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                      "],time:'2024-09-06T23:12:09.139Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['332','0','359','92','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.154Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','360','50','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:"],time:'2024-09-06T23:12:09.158Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['333','0','360','50','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.171Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','361','169','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)"],time:'2024-09-06T23:12:09.174Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['333','0','361','169','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.188Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','362','48','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:"],time:'2024-09-06T23:12:09.193Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['334','0','361','169','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.203Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','362','22','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                      "],time:'2024-09-06T23:12:09.206Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['335','0','362','48','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.219Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','363','64','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'"],time:'2024-09-06T23:12:09.224Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['336','0','363','64','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.236Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','364','32','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:"],time:'2024-09-06T23:12:09.243Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['336','0','364','32','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.253Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','364','22','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_val"],time:'2024-09-06T23:12:09.255Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['337','0','364','32','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.269Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','365','22','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% "],time:'2024-09-06T23:12:09.274Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['338','0','365','86','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.286Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','366','41','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:"],time:'2024-09-06T23:12:09.297Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['339','0','366','41','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.305Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','367','30','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:"],time:'2024-09-06T23:12:09.312Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['340','0','367','30','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.321Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','368','59','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)"],time:'2024-09-06T23:12:09.329Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','368','22','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                      "],time:'2024-09-06T23:12:09.338Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['341','0','368','59','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.353Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','369','80','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())"],time:'2024-09-06T23:12:09.357Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['342','0','369','80','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.370Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','370','40','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:"],time:'2024-09-06T23:12:09.377Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['342','0','370','40','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.387Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','371','111','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))"],time:'2024-09-06T23:12:09.392Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['343','0','370','40','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.406Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','371','22','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                      "],time:'2024-09-06T23:12:09.411Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['344','0','371','111','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.420Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','372','89','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed"],time:'2024-09-06T23:12:09.425Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['345','0','372','89','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.437Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','373','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n"],time:'2024-09-06T23:12:09.441Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['346','0','373','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.466Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','374','96','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative"],time:'2024-09-06T23:12:09.473Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['347','0','374','96','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.482Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','375','54','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:"],time:'2024-09-06T23:12:09.486Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['348','0','375','54','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.500Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','376','173','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)"],time:'2024-09-06T23:12:09.503Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','376','22','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                      "],time:'2024-09-06T23:12:09.514Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['348','0','376','173','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.519Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','377','52','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:"],time:'2024-09-06T23:12:09.524Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['349','0','376','173','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.536Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','377','22','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                      "],time:'2024-09-06T23:12:09.540Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['350','0','377','52','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.553Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','378','68','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'"],time:'2024-09-06T23:12:09.561Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['351','0','378','68','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.569Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','379','21','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:"],time:'2024-09-06T23:12:09.573Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['351','0','379','21','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.585Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','380','161','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)"],time:'2024-09-06T23:12:09.589Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['352','0','379','21','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.602Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','380','22','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    se"],time:'2024-09-06T23:12:09.605Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['353','0','380','161','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.619Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','381','17','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:"],time:'2024-09-06T23:12:09.622Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['354','0','381','17','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.636Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','382','69','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling"],time:'2024-09-06T23:12:09.640Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['354','0','382','69','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.653Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','383','157','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)"],time:'2024-09-06T23:12:09.656Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['355','0','382','69','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.669Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','383','22','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                select"],time:'2024-09-06T23:12:09.672Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['356','0','383','157','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.686Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','384','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n"],time:'2024-09-06T23:12:09.690Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['357','0','384','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.703Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','385','57','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo"],time:'2024-09-06T23:12:09.707Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['358','0','385','57','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.725Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','386','82','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']"],time:'2024-09-06T23:12:09.730Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['358','0','386','82','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.739Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','386','22','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels"],time:'2024-09-06T23:12:09.741Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['359','0','386','82','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.757Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','387','22','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels"],time:'2024-09-06T23:12:09.760Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['360','0','387','82','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.769Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','388','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n"],time:'2024-09-06T23:12:09.773Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['361','0','388','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.790Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','389','64','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)"],time:'2024-09-06T23:12:09.793Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','389','22','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labe"],time:'2024-09-06T23:12:09.806Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['362','0','389','64','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.820Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','390','33','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()"],time:'2024-09-06T23:12:09.823Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['363','0','390','33','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.836Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','391','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n"],time:'2024-09-06T23:12:09.840Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['364','0','391','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.853Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','392','22','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:"],time:'2024-09-06T23:12:09.857Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['364','0','392','22','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.872Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['365','0','392','22','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.886Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','393','51','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)"],time:'2024-09-06T23:12:09.890Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['366','0','393','51','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.903Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','394','61','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)"],time:'2024-09-06T23:12:09.907Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['367','0','394','61','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.919Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','395','74','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)"],time:'2024-09-06T23:12:09.923Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','395','22','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percen"],time:'2024-09-06T23:12:09.938Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['368','0','395','74','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.953Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','396','87','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)"],time:'2024-09-06T23:12:09.957Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['369','0','396','87','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.969Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','397','47','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)"],time:'2024-09-06T23:12:09.973Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['370','0','397','47','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:09.986Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','398','90','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)"],time:'2024-09-06T23:12:09.990Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','398','22','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number"],time:'2024-09-06T23:12:10.006Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','398','90','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)"],time:'2024-09-06T23:12:10.020Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['370','0','398','90','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:10.020Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','398','23','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_"],time:'2024-09-06T23:12:10.070Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','398','90','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)"],time:'2024-09-06T23:12:10.836Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','398','22','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number"],time:'2024-09-06T23:12:11.110Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['371','0','398','90','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:11.136Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','399','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n"],time:'2024-09-06T23:12:11.136Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['372','0','399','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:11.153Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','400','48','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:"],time:'2024-09-06T23:12:11.157Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','400','22','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num"],time:'2024-09-06T23:12:11.172Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['373','0','400','48','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:11.186Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','401','91','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold"],time:'2024-09-06T23:12:11.190Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['373','0','401','91','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:11.203Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','402','21','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:"],time:'2024-09-06T23:12:11.206Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['374','0','401','91','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:11.219Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['375','0','402','21','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:11.236Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','403','35','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0"],time:'2024-09-06T23:12:11.240Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','403','22','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    co"],time:'2024-09-06T23:12:11.258Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['376','0','403','35','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:11.271Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','404','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n"],time:'2024-09-06T23:12:11.278Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['376','0','404','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:11.286Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','405','70','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold"],time:'2024-09-06T23:12:11.290Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['377','0','404','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:11.303Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['378','0','405','70','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:11.319Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','406','33','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:"],time:'2024-09-06T23:12:11.324Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','406','22','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num"],time:'2024-09-06T23:12:11.338Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['379','0','406','33','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:11.356Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','407','82','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100"],time:'2024-09-06T23:12:11.359Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['379','0','407','82','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:11.371Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','408','21','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:"],time:'2024-09-06T23:12:11.373Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['380','0','407','82','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:11.386Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['381','0','408','21','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:11.403Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','409','39','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0"],time:'2024-09-06T23:12:11.406Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','409','22','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    ru"],time:'2024-09-06T23:12:11.422Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['382','0','409','39','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:11.436Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','410','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n"],time:'2024-09-06T23:12:11.439Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['382','0','410','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:11.466Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','411','94','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)"],time:'2024-09-06T23:12:11.466Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['383','0','410','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:11.472Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','411','22','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_p"],time:'2024-09-06T23:12:11.476Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['384','0','411','94','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:11.491Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','412','86','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)"],time:'2024-09-06T23:12:11.496Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['385','0','412','86','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:11.507Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','413','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n"],time:'2024-09-06T23:12:11.510Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['386','0','413','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:11.536Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','414','53','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration"],time:'2024-09-06T23:12:11.539Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','414','22','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculat"],time:'2024-09-06T23:12:11.555Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['387','0','414','53','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:11.570Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','415','71','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)"],time:'2024-09-06T23:12:11.574Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['387','0','415','71','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:11.586Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','416','76','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)"],time:'2024-09-06T23:12:11.591Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['388','0','415','71','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:11.603Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['389','0','416','76','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:11.620Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','417','75','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')"],time:'2024-09-06T23:12:11.625Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','417','22','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Ac"],time:'2024-09-06T23:12:11.639Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['390','0','417','75','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:11.653Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','418','108','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')"],time:'2024-09-06T23:12:11.657Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['390','0','418','108','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:11.669Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','419','80','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')"],time:'2024-09-06T23:12:11.673Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['391','0','418','108','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:11.687Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','419','22','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Co"],time:'2024-09-06T23:12:11.690Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['392','0','419','80','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:11.703Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','420','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n"],time:'2024-09-06T23:12:11.706Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['392','0','420','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:11.721Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['393','0','420','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:11.736Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','421','32','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Save the results to a csv file"],time:'2024-09-06T23:12:11.739Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['394','0','421','32','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:11.753Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','422','62','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Save the results to a csv file\nwith open(warning_type + '_simulation_results.csv', 'w') as f:"],time:'2024-09-06T23:12:11.756Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','422','22','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Save the results to a csv file\nwith open(warning_type"],time:'2024-09-06T23:12:11.773Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['395','0','422','62','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:11.786Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','423','84','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Save the results to a csv file\nwith open(warning_type + '_simulation_results.csv', 'w') as f:\n    f.write('Scenario,Probability,Iteration,Accuracy,Rule Percentage,Conciseness\\n')"],time:'2024-09-06T23:12:11.790Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['395','0','423','84','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:11.803Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','424','61','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Save the results to a csv file\nwith open(warning_type + '_simulation_results.csv', 'w') as f:\n    f.write('Scenario,Probability,Iteration,Accuracy,Rule Percentage,Conciseness\\n')\n    for scenario_name, p_results in accuracy_results.items():"],time:'2024-09-06T23:12:11.806Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['396','0','423','84','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:11.819Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','424','22','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Save the results to a csv file\nwith open(warning_type + '_simulation_results.csv', 'w') as f:\n    f.write('Scenario,Probability,Iteration,Accuracy,Rule Percentage,Conciseness\\n')\n    for scenario_name,"],time:'2024-09-06T23:12:11.822Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['397','0','424','61','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:11.836Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','425','56','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Save the results to a csv file\nwith open(warning_type + '_simulation_results.csv', 'w') as f:\n    f.write('Scenario,Probability,Iteration,Accuracy,Rule Percentage,Conciseness\\n')\n    for scenario_name, p_results in accuracy_results.items():\n        for p_value, accuracy_list in p_results.items():"],time:'2024-09-06T23:12:11.840Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','425','22','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Save the results to a csv file\nwith open(warning_type + '_simulation_results.csv', 'w') as f:\n    f.write('Scenario,Probability,Iteration,Accuracy,Rule Percentage,Conciseness\\n')\n    for scenario_name, p_results in accuracy_results.items():\n        for p_value, a"],time:'2024-09-06T23:12:11.855Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['398','0','425','56','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:11.870Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','426','82','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Save the results to a csv file\nwith open(warning_type + '_simulation_results.csv', 'w') as f:\n    f.write('Scenario,Probability,Iteration,Accuracy,Rule Percentage,Conciseness\\n')\n    for scenario_name, p_results in accuracy_results.items():\n        for p_value, accuracy_list in p_results.items():\n            rule_percentage_list = rule_percentage_results[scenario_name][p_value]"],time:'2024-09-06T23:12:11.873Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['399','0','426','82','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:11.886Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','427','74','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Save the results to a csv file\nwith open(warning_type + '_simulation_results.csv', 'w') as f:\n    f.write('Scenario,Probability,Iteration,Accuracy,Rule Percentage,Conciseness\\n')\n    for scenario_name, p_results in accuracy_results.items():\n        for p_value, accuracy_list in p_results.items():\n            rule_percentage_list = rule_percentage_results[scenario_name][p_value]\n            conciseness_list = conciseness_results[scenario_name][p_value]"],time:'2024-09-06T23:12:11.890Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','427','22','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Save the results to a csv file\nwith open(warning_type + '_simulation_results.csv', 'w') as f:\n    f.write('Scenario,Probability,Iteration,Accuracy,Rule Percentage,Conciseness\\n')\n    for scenario_name, p_results in accuracy_results.items():\n        for p_value, accuracy_list in p_results.items():\n            rule_percentage_list = rule_percentage_results[scenario_name][p_value]\n            concisenes"],time:'2024-09-06T23:12:11.906Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['400','0','427','74','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:11.922Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','428','47','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Save the results to a csv file\nwith open(warning_type + '_simulation_results.csv', 'w') as f:\n    f.write('Scenario,Probability,Iteration,Accuracy,Rule Percentage,Conciseness\\n')\n    for scenario_name, p_results in accuracy_results.items():\n        for p_value, accuracy_list in p_results.items():\n            rule_percentage_list = rule_percentage_results[scenario_name][p_value]\n            conciseness_list = conciseness_results[scenario_name][p_value]\n            for i in range(len(accuracy_list)):"],time:'2024-09-06T23:12:11.926Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['400','0','428','47','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:11.937Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','429','79','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Save the results to a csv file\nwith open(warning_type + '_simulation_results.csv', 'w') as f:\n    f.write('Scenario,Probability,Iteration,Accuracy,Rule Percentage,Conciseness\\n')\n    for scenario_name, p_results in accuracy_results.items():\n        for p_value, accuracy_list in p_results.items():\n            rule_percentage_list = rule_percentage_results[scenario_name][p_value]\n            conciseness_list = conciseness_results[scenario_name][p_value]\n            for i in range(len(accuracy_list)):\n                if i < len(rule_percentage_list) and i < len(conciseness_list):"],time:'2024-09-06T23:12:11.942Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['401','0','428','47','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:11.952Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['402','0','429','79','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:11.970Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','430','130','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Save the results to a csv file\nwith open(warning_type + '_simulation_results.csv', 'w') as f:\n    f.write('Scenario,Probability,Iteration,Accuracy,Rule Percentage,Conciseness\\n')\n    for scenario_name, p_results in accuracy_results.items():\n        for p_value, accuracy_list in p_results.items():\n            rule_percentage_list = rule_percentage_results[scenario_name][p_value]\n            conciseness_list = conciseness_results[scenario_name][p_value]\n            for i in range(len(accuracy_list)):\n                if i < len(rule_percentage_list) and i < len(conciseness_list):\n                    f.write(f'{scenario_name},{p_value},{i},{accuracy_list[i]},{rule_percentage_list[i]},{conciseness_list[i]}\\n')"],time:'2024-09-06T23:12:11.975Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['403','0','430','130','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:12:12.006Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','431','21','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Save the results to a csv file\nwith open(warning_type + '_simulation_results.csv', 'w') as f:\n    f.write('Scenario,Probability,Iteration,Accuracy,Rule Percentage,Conciseness\\n')\n    for scenario_name, p_results in accuracy_results.items():\n        for p_value, accuracy_list in p_results.items():\n            rule_percentage_list = rule_percentage_results[scenario_name][p_value]\n            conciseness_list = conciseness_results[scenario_name][p_value]\n            for i in range(len(accuracy_list)):\n                if i < len(rule_percentage_list) and i < len(conciseness_list):\n                    f.write(f'{scenario_name},{p_value},{i},{accuracy_list[i]},{rule_percentage_list[i]},{conciseness_list[i]}\\n')\n                else:"],time:'2024-09-06T23:12:12.013Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','430','23','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Save the results to a csv file\nwith open(warning_type + '_simulation_results.csv', 'w') as f:\n    f.write('Scenario,Probability,Iteration,Accuracy,Rule Percentage,Conciseness\\n')\n    for scenario_name, p_results in accuracy_results.items():\n        for p_value, accuracy_list in p_results.items():\n            rule_percentage_list = rule_percentage_results[scenario_name][p_value]\n            conciseness_list = conciseness_results[scenario_name][p_value]\n            for i in range(len(accuracy_list)):\n                if i < len(rule_percentage_list) and i < len(conciseness_list):\n                    f.w"],time:'2024-09-06T23:12:12.070Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','429','23','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Save the results to a csv file\nwith open(warning_type + '_simulation_results.csv', 'w') as f:\n    f.write('Scenario,Probability,Iteration,Accuracy,Rule Percentage,Conciseness\\n')\n    for scenario_name, p_results in accuracy_results.items():\n        for p_value, accuracy_list in p_results.items():\n            rule_percentage_list = rule_percentage_results[scenario_name][p_value]\n            conciseness_list = conciseness_results[scenario_name][p_value]\n            for i in range(len(accuracy_list)):\n                if i < "],time:'2024-09-06T23:12:12.636Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','428','24','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Save the results to a csv file\nwith open(warning_type + '_simulation_results.csv', 'w') as f:\n    f.write('Scenario,Probability,Iteration,Accuracy,Rule Percentage,Conciseness\\n')\n    for scenario_name, p_results in accuracy_results.items():\n        for p_value, accuracy_list in p_results.items():\n            rule_percentage_list = rule_percentage_results[scenario_name][p_value]\n            conciseness_list = conciseness_results[scenario_name][p_value]\n            for i in ran"],time:'2024-09-06T23:12:12.720Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','427','24','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Save the results to a csv file\nwith open(warning_type + '_simulation_results.csv', 'w') as f:\n    f.write('Scenario,Probability,Iteration,Accuracy,Rule Percentage,Conciseness\\n')\n    for scenario_name, p_results in accuracy_results.items():\n        for p_value, accuracy_list in p_results.items():\n            rule_percentage_list = rule_percentage_results[scenario_name][p_value]\n            conciseness_"],time:'2024-09-06T23:12:12.753Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','426','25','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Save the results to a csv file\nwith open(warning_type + '_simulation_results.csv', 'w') as f:\n    f.write('Scenario,Probability,Iteration,Accuracy,Rule Percentage,Conciseness\\n')\n    for scenario_name, p_results in accuracy_results.items():\n        for p_value, accuracy_list in p_results.items():\n            rule_percenta"],time:'2024-09-06T23:12:12.803Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','425','25','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Save the results to a csv file\nwith open(warning_type + '_simulation_results.csv', 'w') as f:\n    f.write('Scenario,Probability,Iteration,Accuracy,Rule Percentage,Conciseness\\n')\n    for scenario_name, p_results in accuracy_results.items():\n        for p_value, accu"],time:'2024-09-06T23:12:12.837Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','425','26','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Save the results to a csv file\nwith open(warning_type + '_simulation_results.csv', 'w') as f:\n    f.write('Scenario,Probability,Iteration,Accuracy,Rule Percentage,Conciseness\\n')\n    for scenario_name, p_results in accuracy_results.items():\n        for p_value, accur"],time:'2024-09-06T23:12:12.852Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','424','26','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Save the results to a csv file\nwith open(warning_type + '_simulation_results.csv', 'w') as f:\n    f.write('Scenario,Probability,Iteration,Accuracy,Rule Percentage,Conciseness\\n')\n    for scenario_name, p_r"],time:'2024-09-06T23:12:12.886Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','424','27','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Save the results to a csv file\nwith open(warning_type + '_simulation_results.csv', 'w') as f:\n    f.write('Scenario,Probability,Iteration,Accuracy,Rule Percentage,Conciseness\\n')\n    for scenario_name, p_re"],time:'2024-09-06T23:12:12.970Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','423','27','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Save the results to a csv file\nwith open(warning_type + '_simulation_results.csv', 'w') as f:\n    f.write('Scenario,Proba"],time:'2024-09-06T23:12:13.003Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','423','28','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Save the results to a csv file\nwith open(warning_type + '_simulation_results.csv', 'w') as f:\n    f.write('Scenario,Probab"],time:'2024-09-06T23:12:13.053Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','422','28','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Save the results to a csv file\nwith open(warning_type + '_s"],time:'2024-09-06T23:12:13.086Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','422','29','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Save the results to a csv file\nwith open(warning_type + '_si"],time:'2024-09-06T23:12:13.136Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','421','29','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Save the results to a csv f"],time:'2024-09-06T23:12:13.253Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','421','30','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Save the results to a csv fi"],time:'2024-09-06T23:12:13.286Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','421','31','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n\n# Save the results to a csv fil"],time:'2024-09-06T23:12:13.353Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','420','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n"],time:'2024-09-06T23:12:13.370Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','419','33','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness a"],time:'2024-09-06T23:12:13.602Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','419','34','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness af"],time:'2024-09-06T23:12:13.670Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','419','35','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness aft"],time:'2024-09-06T23:12:15.387Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','419','36','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness afte"],time:'2024-09-06T23:12:15.420Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','419','37','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after"],time:'2024-09-06T23:12:15.453Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','419','38','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after "],time:'2024-09-06T23:12:15.470Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','419','39','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after i"],time:'2024-09-06T23:12:15.486Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','419','41','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after ite"],time:'2024-09-06T23:12:15.503Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','419','43','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after itera"],time:'2024-09-06T23:12:15.520Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','420','0','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')\n"],time:'2024-09-06T23:12:15.536Z'},{src:'onDidChangeTextEditorSelection',msg:'%s:%s to %s:%s in [%s] text: %s',prm:['321','0','419','80','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py',"for scenario_name, heuristics in scenarios.items():\n    rule_percentage = 0\n    conciseness = 0\n    accuracy = 0\n    for p_value in [0, 0.5, 1]:\n        print(f\"Running scenario: {scenario_name} with p={p_value}\")\n\n        warnings_state = initialize_warnings_state(ground_truth)\n        num_warnings = len(ground_truth)\n\n        output = run_clingo()\n\n        # get the first warning\n        selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, initialize_warnings_state(ground_truth), apply_heuristics=heuristics)\n        for pos in selected_pos:\n            warnings_state[pos] = 'positive'\n\n        # Run simulation for # of warnings iterations\n        for iteration in range(num_warnings - 1):\n            if all([v != 'uninspected' for v in warnings_state.values()]):\n                # fill in the rest of the warnings with the last accuracy, rule percentage, and conciseness\n                for i in range(iteration, num_warnings):\n                    accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n                    rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                    conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n                break\n            \n            print(f'Simulation Iteration {iteration} for {scenario_name} with p={p_value}')\n\n            if p_value == 1:\n                # Use the rules to select positive/negative warnings\n                if output:\n                    model = parse_clingo_output(output)\n                    rule_numbers = list(extract_summary_rules(model).keys())\n                    if rule_numbers:\n                        selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                        selected_neg = []  # Handle negative case similarly if needed\n\n                        # 0.1 probability to mark all matching warnings as positive/negative\n                        if random.random() > 0.05:\n                            selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                        for pos in selected_pos:\n                                warnings_state[pos] = 'positive'\n            elif p_value == 0.5:\n                # 50% chance to use the rules or the original heuristic-based sampling\n                if random.random() < 0.5:\n                    if output:\n                        model = parse_clingo_output(output)\n                        rule_numbers = list(extract_summary_rules(model).keys())\n                        if rule_numbers:\n                            selected_pos = get_positive_predictions_of_rule(model, random.choice(rule_numbers))\n                            selected_neg = []  # Handle negative case similarly if needed\n\n                            # 0.1 probability to mark all matching warnings as positive/negative\n                            if random.random() > 0.05:\n                                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n                            for pos in selected_pos:\n                                    warnings_state[pos] = 'positive'\n                else:\n                    selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n            else:\n                # p=0, just use the original heuristic-based sampling\n                selected_pos, selected_neg = sample_labels_randomized_then_sorted(ground_truth, 1, 1, code_data, warnings_state, apply_heuristics=heuristics)\n\n            # Write labels to Clingo input and run Clingo\n            pos_labels = [k for k, v in warnings_state.items() if v == 'positive']\n            neg_labels = [k for k, v in warnings_state.items() if v == 'negative']\n\n            write_labels_to_clingo_input(pos_labels, neg_labels)\n            output = run_clingo()\n\n            if output:\n                model = parse_clingo_output(output)\n                inferred_rules = extract_summary_rules(model)\n                percentages = calculate_rule_percentage(model, pos_labels)\n                num_rules_over_threshold = number_of_rules_over_percentage(percentages)\n                num_rules = len(inferred_rules)\n                number_of_positive_predictions = get_number_of_positive_predictions(model)\n\n                if num_rules_over_threshold > 0:\n                    conciseness = number_of_positive_predictions / num_rules_over_threshold\n                else:\n                    conciseness = 0\n\n                # Calculate the percentage of rules over the threshold\n                if num_rules > 0:\n                    rule_percentage = (num_rules_over_threshold / num_rules) * 100\n                else:\n                    rule_percentage = 0\n\n                rule_percentage_results[scenario_name][f'p={p_value}'].append(rule_percentage)\n                conciseness_results[scenario_name][f'p={p_value}'].append(conciseness)\n\n            # Calculate accuracy after each iteration\n            accuracy = calculate_accuracy(warnings_state, ground_truth)\n            accuracy_results[scenario_name][f'p={p_value}'].append(accuracy)\n            print(f'Accuracy after iteration {iteration}: {accuracy:.2f}%')\n            print(f'Percentage of rules over threshold after iteration {iteration}: {rule_percentage:.2f}%')\n            print(f'Conciseness after iteration {iteration}: {conciseness:.2f}')"],time:'2024-09-06T23:12:16.120Z'},{src:'onDidChangeTextEditorVisibleRanges',msg:'%s:%s to %s:%s [%s]',prm:['395','0','422','62','/Users/burakyetistiren/Desktop/warning_suppression/code/simulate.py'],time:'2024-09-06T23:19:16.442Z'}]